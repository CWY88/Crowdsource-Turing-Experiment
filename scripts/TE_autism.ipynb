{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b32d9f-f0a1-43f9-b7d1-a8ea55b8946d",
   "metadata": {},
   "source": [
    "### 0. Import Settings\n",
    "#### Import necessary functions and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35cf91c-e4d9-4f59-9a04-634bf5f49029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "# Add src module to path before import.\n",
    "sys.path.insert(0, str(pathlib.Path('../src')))\n",
    "from file_IO_handler import get_plaintext_file_contents\n",
    "from openai_handler import (\n",
    "    verify_openai_access, \n",
    "    OpenAIModelSettings, \n",
    "    MODELS,  # Changed from ENGINES to MODELS\n",
    "    call_openai_chat_api  # Changed from call_openai_api to call_openai_chat_api\n",
    ")\n",
    "from fill_string_template import get_filled_strings_from_dataframe, FilledString\n",
    "from run_simulation import run_single_simulation, save_simulation_result_to_unique_location\n",
    "from process_results import (\n",
    "    consolidate_jsons_to_mega_json, \n",
    "    process_mega_json_for_no_complete_prompt, \n",
    "    consolidate_jsons_to_mega_json_by_engine_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1779b5c-9048-436d-8612-f52b28842580",
   "metadata": {},
   "source": [
    "### 1. Prior Settings\n",
    "#### 1.1 Import the filtered 20 questions and 100 randomly selected outgroup answers\n",
    "##### Read the 2 csv files into dataframe, show basic structure and first few lines and demonstrate all the column numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8f5de2-06c7-43b7-a7f8-f49402065e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading outgroup_answers.csv ===\n",
      "Data shape: (100, 5)\n",
      "Column names: ['Input.title', 'Input.body', 'Answer.Confidence', 'Answer.reason for confidence rating', 'Answer.answer']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input.title</th>\n",
       "      <th>Input.body</th>\n",
       "      <th>Answer.Confidence</th>\n",
       "      <th>Answer.reason for confidence rating</th>\n",
       "      <th>Answer.answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anyone have Skype?</td>\n",
       "      <td>Well, like the subject says, anyone have Skype...</td>\n",
       "      <td>Somewhat confident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phone conversations can be difficult even for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone have Skype?</td>\n",
       "      <td>Well, like the subject says, anyone have Skype...</td>\n",
       "      <td>Somewhat confident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>skype is a good tool for interaction with thos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyone have Skype?</td>\n",
       "      <td>Well, like the subject says, anyone have Skype...</td>\n",
       "      <td>Very confident</td>\n",
       "      <td>Experience with Skype</td>\n",
       "      <td>I definitely think that utilizing Skype for ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anyone have Skype?</td>\n",
       "      <td>Well, like the subject says, anyone have Skype...</td>\n",
       "      <td>Somewhat confident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rather than skype, a website called compassion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anyone have Skype?</td>\n",
       "      <td>Well, like the subject says, anyone have Skype...</td>\n",
       "      <td>Somewhat confident</td>\n",
       "      <td>Skype is so popular and widely distributed tha...</td>\n",
       "      <td>Yes, i do and so do many others, especially si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Input.title                                         Input.body  \\\n",
       "0  Anyone have Skype?  Well, like the subject says, anyone have Skype...   \n",
       "1  Anyone have Skype?  Well, like the subject says, anyone have Skype...   \n",
       "2  Anyone have Skype?  Well, like the subject says, anyone have Skype...   \n",
       "3  Anyone have Skype?  Well, like the subject says, anyone have Skype...   \n",
       "4  Anyone have Skype?  Well, like the subject says, anyone have Skype...   \n",
       "\n",
       "    Answer.Confidence                Answer.reason for confidence rating  \\\n",
       "0  Somewhat confident                                                NaN   \n",
       "1  Somewhat confident                                                NaN   \n",
       "2      Very confident                              Experience with Skype   \n",
       "3  Somewhat confident                                                NaN   \n",
       "4  Somewhat confident  Skype is so popular and widely distributed tha...   \n",
       "\n",
       "                                       Answer.answer  \n",
       "0  Phone conversations can be difficult even for ...  \n",
       "1  skype is a good tool for interaction with thos...  \n",
       "2  I definitely think that utilizing Skype for ot...  \n",
       "3  Rather than skype, a website called compassion...  \n",
       "4  Yes, i do and so do many others, especially si...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "=== Reading questions.csv ===\n",
      "Data shape: (20, 2)\n",
      "Column names: ['Title', 'Body']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dr Who, is he Autistic?</td>\n",
       "      <td>I watch Dr Who each week and I keep picking ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haldol and Cogentin (What is your Medication)?</td>\n",
       "      <td>Anyone here ever take Haldol or Cogentin?  Als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Problem with friendships...</td>\n",
       "      <td>Ever since I was a child, I've been overly att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you cope with power cuts?</td>\n",
       "      <td>Just recently, I've been having power cuts at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Problems with phones</td>\n",
       "      <td>Hi there - i am new to this forum so bear with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title  \\\n",
       "0                         Dr Who, is he Autistic?   \n",
       "1  Haldol and Cogentin (What is your Medication)?   \n",
       "2                     Problem with friendships...   \n",
       "3                How do you cope with power cuts?   \n",
       "4                            Problems with phones   \n",
       "\n",
       "                                                Body  \n",
       "0  I watch Dr Who each week and I keep picking ou...  \n",
       "1  Anyone here ever take Haldol or Cogentin?  Als...  \n",
       "2  Ever since I was a child, I've been overly att...  \n",
       "3  Just recently, I've been having power cuts at ...  \n",
       "4  Hi there - i am new to this forum so bear with...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "=== Data Overview ===\n",
      "outgroup_answers.csv data types:\n",
      "Input.title                            object\n",
      "Input.body                             object\n",
      "Answer.Confidence                      object\n",
      "Answer.reason for confidence rating    object\n",
      "Answer.answer                          object\n",
      "dtype: object\n",
      "\n",
      "Missing values count:\n",
      "Input.title                             0\n",
      "Input.body                              0\n",
      "Answer.Confidence                       1\n",
      "Answer.reason for confidence rating    47\n",
      "Answer.answer                           0\n",
      "dtype: int64\n",
      "\n",
      "------------------------------\n",
      "\n",
      "questions.csv data types:\n",
      "Title    object\n",
      "Body     object\n",
      "dtype: object\n",
      "\n",
      "Missing values count:\n",
      "Title    0\n",
      "Body     0\n",
      "dtype: int64\n",
      "\n",
      "Data extraction completed!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set file path\n",
    "file_path = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\Out-group QA\"\n",
    "\n",
    "# Read the first CSV file: outgroup_answers.csv\n",
    "print(\"=== Reading outgroup_answers.csv ===\")\n",
    "outgroup_df = pd.read_csv(f\"{file_path}\\\\outgroup_answers.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Data shape: {outgroup_df.shape}\")\n",
    "print(f\"Column names: {list(outgroup_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(outgroup_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Read the second CSV file: questions.csv  \n",
    "print(\"=== Reading questions.csv ===\")\n",
    "questions_df = pd.read_csv(f\"{file_path}\\\\questions.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Data shape: {questions_df.shape}\")\n",
    "print(f\"Column names: {list(questions_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(questions_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Data overview\n",
    "print(\"=== Data Overview ===\")\n",
    "print(\"outgroup_answers.csv data types:\")\n",
    "print(outgroup_df.dtypes)\n",
    "print(f\"\\nMissing values count:\")\n",
    "print(outgroup_df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"questions.csv data types:\")\n",
    "print(questions_df.dtypes)\n",
    "print(f\"\\nMissing values count:\")\n",
    "print(questions_df.isnull().sum())\n",
    "\n",
    "# Optional: Save processed data to new files\n",
    "# outgroup_df.to_csv(f\"{file_path}\\\\outgroup_answers_processed.csv\", index=False)\n",
    "# questions_df.to_csv(f\"{file_path}\\\\questions_processed.csv\", index=False)\n",
    "\n",
    "print(\"\\nData extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da1680-c99b-4b19-ad94-74cf97397966",
   "metadata": {},
   "source": [
    "#### 1.2 Calculate average length for the out-group answers\n",
    "##### This calculated average length will be used as a length limitation for later prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfef8cdb-09f1-48d3-bd86-7cacc05e258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Answer Analysis ===\n",
      "Total number of valid answers: 100\n",
      "Average word length per answer: 49.27 words\n",
      "Min word count: 3\n",
      "Max word count: 237\n",
      "Standard deviation: 40.21\n",
      "\n",
      "Word count distribution:\n",
      "count    100.000000\n",
      "mean      49.270000\n",
      "std       40.210521\n",
      "min        3.000000\n",
      "25%       22.750000\n",
      "50%       35.500000\n",
      "75%       68.250000\n",
      "max      237.000000\n",
      "Name: Answer.answer, dtype: float64\n",
      "\n",
      "Answer analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# Calculate average word length for Answer.answer column\n",
    "print(\"=== Answer Analysis ===\")\n",
    "\n",
    "# Remove NaN values and calculate word count for each answer\n",
    "valid_answers = outgroup_df['Answer.answer'].dropna()\n",
    "word_counts = valid_answers.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Calculate average word length\n",
    "avg_word_length = word_counts.mean()\n",
    "\n",
    "# Store in variable and display results\n",
    "average_answer_word_length = avg_word_length\n",
    "\n",
    "print(f\"Total number of valid answers: {len(valid_answers)}\")\n",
    "print(f\"Average word length per answer: {average_answer_word_length:.2f} words\")\n",
    "print(f\"Min word count: {word_counts.min()}\")\n",
    "print(f\"Max word count: {word_counts.max()}\")\n",
    "print(f\"Standard deviation: {word_counts.std():.2f}\")\n",
    "\n",
    "# Display some statistics about word distribution\n",
    "print(f\"\\nWord count distribution:\")\n",
    "print(word_counts.describe())\n",
    "\n",
    "print(\"\\nAnswer analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131316da-987a-45fa-a296-89d4ef4c1d16",
   "metadata": {},
   "source": [
    "#### 1.3 LM parameter settings and LM selection\n",
    "##### Determine the appropriate LM parameter settings and test valid LM selected for our parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5823f48a-a4e1-4e81-9af0-a3e772479731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Model Testing Script\n",
      "==================================================\n",
      "OpenAI client initialized successfully\n",
      "✓ Model gpt-4o: Working\n",
      "✓ Model gpt-4: Working\n",
      "✓ Model gpt-4-turbo: Working\n",
      "✓ Model gpt-3.5-turbo: Working\n",
      "✓ Model gpt-3.5-turbo-16k: Working\n",
      "\n",
      "Testing completed!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "# Assume you have these functions defined in your module\n",
    "# from your_module import verify_openai_access, OpenAIModelSettings, call_openai_api\n",
    "\n",
    "class OpenAIModelSettings:\n",
    "    \"\"\"Model settings class for v1/chat/completions API\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",  # Changed from 'engine' to 'model'\n",
    "        max_tokens: int = 1000,\n",
    "        temperature: float = 0.3,\n",
    "        n: int = 1,\n",
    "        presence_penalty: float = 0.1,\n",
    "        frequency_penalty: float = 0.1,\n",
    "        stop: List[str] = None,\n",
    "        params_descriptor: str = \"autism-community-response\"\n",
    "    ):\n",
    "        self.model = model  # Chat completions uses 'model' instead of 'engine'\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.n = n\n",
    "        self.presence_penalty = presence_penalty\n",
    "        self.frequency_penalty = frequency_penalty\n",
    "        self.stop = stop\n",
    "        self.params_descriptor = params_descriptor\n",
    "        \n",
    "    def to_chat_completion_params(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert settings to chat completion API parameters\"\"\"\n",
    "        params = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"n\": self.n,\n",
    "            \"presence_penalty\": self.presence_penalty,\n",
    "            \"frequency_penalty\": self.frequency_penalty,\n",
    "        }\n",
    "        \n",
    "        if self.stop is not None:\n",
    "            params[\"stop\"] = self.stop\n",
    "            \n",
    "        return params\n",
    "\n",
    "def call_openai_chat_api(prompt: str, model_settings: OpenAIModelSettings, client) -> str:\n",
    "    \"\"\"Call OpenAI Chat Completions API\"\"\"\n",
    "    # Convert prompt to chat format\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Get API parameters\n",
    "    params = model_settings.to_chat_completion_params(messages)\n",
    "    \n",
    "    try:\n",
    "        # Call the chat completions endpoint\n",
    "        response = client.chat.completions.create(**params)\n",
    "        \n",
    "        # Extract the response content\n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def test_multiple_openai_models():\n",
    "    \"\"\"Function to test multiple OpenAI models\"\"\"\n",
    "    \n",
    "    # List of available OpenAI models to test\n",
    "    models_to_test = [\n",
    "        \"gpt-4o\",\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4-turbo\",\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo-16k\"\n",
    "    ]\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"Q: How many legs does a cat have?\",\n",
    "        \"Q: What is the capital of France?\",\n",
    "        \"Q: Explain photosynthesis in simple terms.\",\n",
    "        \"Q: What are the benefits of exercise?\",\n",
    "        \"Q: How do you make a paper airplane?\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    try:\n",
    "        client = verify_openai_access(\n",
    "            pathlib.Path(\"openai_organization.txt\"),\n",
    "            pathlib.Path(\"openai_api_key.txt\")\n",
    "        )\n",
    "        print(\"OpenAI client initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize OpenAI client: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test each model\n",
    "    for model in models_to_test:\n",
    "        # Create model settings\n",
    "        model_settings = OpenAIModelSettings(\n",
    "            model=model,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.3,\n",
    "            n=1,\n",
    "            presence_penalty=0.1,\n",
    "            frequency_penalty=0.1,\n",
    "            stop=None,\n",
    "            params_descriptor=\"autism-community-response\"\n",
    "        )\n",
    "        \n",
    "        # Test first prompt only to check if model works\n",
    "        prompt = test_prompts[0]\n",
    "        \n",
    "        try:\n",
    "            # Call Chat Completions API\n",
    "            response = call_openai_chat_api(prompt, model_settings, client)\n",
    "            print(f\"✓ Model {model}: Working\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Model {model}: Error - {e}\")\n",
    "        \n",
    "        # Add delay to avoid API rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "def test_single_model(model_name: str, custom_prompt: str = None):\n",
    "    \"\"\"Function to test a single model\"\"\"\n",
    "    print(f\"Testing single model: {model_name}\")\n",
    "    \n",
    "    # Initialize client\n",
    "    try:\n",
    "        client = verify_openai_access(\n",
    "            pathlib.Path(\"openai_organization.txt\"),\n",
    "            pathlib.Path(\"openai_api_key.txt\")\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize client: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Use custom prompt or default prompt\n",
    "    prompt = custom_prompt or \"Q: How many legs does a cat have?\"\n",
    "    \n",
    "    # Create model settings\n",
    "    model_settings = OpenAIModelSettings(\n",
    "        model=model_name,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.3,\n",
    "        n=1,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        stop=None,\n",
    "        params_descriptor=\"autism-community-response\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        start_time = time.time()\n",
    "        response = call_openai_chat_api(prompt, model_settings, client)  # Changed to chat API\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"Response time: {end_time - start_time:.2f}s\")\n",
    "        print(f\"Response: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"OpenAI Model Testing Script\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # # Choose running mode\n",
    "    # print(\"Choose running mode:\")\n",
    "    # print(\"1. Test all models\")\n",
    "    # print(\"2. Test single model\")\n",
    "    \n",
    "    # choice = input(\"Enter your choice (1 or 2): \").strip()\n",
    "    \n",
    "    # if choice == \"1\":\n",
    "        # Test all models\n",
    "    test_multiple_openai_models()\n",
    "    # elif choice == \"2\":\n",
    "    #     # Test single model\n",
    "    #     model_name = input(\"Enter model name (e.g., gpt-3.5-turbo): \").strip()\n",
    "    #     custom_prompt = input(\"Enter custom prompt (press Enter for default): \").strip()\n",
    "    #     test_single_model(model_name, custom_prompt if custom_prompt else None)\n",
    "    # else:\n",
    "    #     print(\"Invalid choice\")\n",
    "    \n",
    "    print(\"\\nTesting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3a7e6-0447-4dcd-bc89-91b06607ff69",
   "metadata": {},
   "source": [
    "### 2. Out-group simulation\n",
    "#### 2.1 Design the demographic distributions based on Hong's study\n",
    "##### We design the demogrphic profile across 3 dimensions and refer to the original study's demographic survey results\n",
    "##### 3 dimensions: Personal Demographic Information + Experience with Autism + Knowledge with Autism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce476aa-6bf0-489c-b5b5-79c6b063c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2.1: Demographics Distribution Design ===\n",
      "Demographic distributions created successfully!\n",
      "Age distribution: {'25-30': 20, '30-35': 35, '35-40': 30, '40-50': 15}\n",
      "Gender distribution: {'female': 51, 'male': 49}\n",
      "Location distribution: {'US': 76, 'Canada': 8, 'UK': 6, 'Australia': 5, 'New_Zealand': 3, 'Ireland': 2}\n",
      "Autism experience distribution: {'caregiver': 33, 'professional': 4, 'regular_interaction': 3, 'some_experience': 30, 'no_experience': 30}\n",
      "Knowledge level distribution: {'none': 6, 'little': 65, 'a_lot': 29}\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Design demographic distributions based on original study\n",
    "print(\"=== Step 2.1: Demographics Distribution Design ===\")\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_demographic_distributions():\n",
    "    \"\"\"Create demographic distributions based on original study data\"\"\"\n",
    "    \n",
    "    # Age distribution to achieve average 33.4 years\n",
    "    age_distribution = {\n",
    "        \"25-30\": 20,    # 20 people, average 27.5 years\n",
    "        \"30-35\": 35,    # 35 people, average 32.5 years  \n",
    "        \"35-40\": 30,    # 30 people, average 37.5 years\n",
    "        \"40-50\": 15     # 15 people, average 45 years\n",
    "    }\n",
    "    # Calculation: (20×27.5 + 35×32.5 + 30×37.5 + 15×45) ÷ 100 = 33.4 years\n",
    "    \n",
    "    # Gender distribution\n",
    "    gender_distribution = {\n",
    "        \"female\": 51,   # 51%\n",
    "        \"male\": 49      # 49%\n",
    "    }\n",
    "    \n",
    "    # Location distribution (English-speaking countries only)\n",
    "    location_distribution = {\n",
    "        \"US\": 76,                    # 76%\n",
    "        \"Canada\": 8,                 # 8%\n",
    "        \"UK\": 6,                     # 6%\n",
    "        \"Australia\": 5,              # 5%\n",
    "        \"New_Zealand\": 3,            # 3%\n",
    "        \"Ireland\": 2                 # 2%\n",
    "    }\n",
    "    \n",
    "    # Autism experience distribution based on study data\n",
    "    # 70% have experience, 40% regular interaction, 33% caregivers, 4% professionals\n",
    "    autism_experience_distribution = {\n",
    "        \"caregiver\": 33,              # 33% caregivers\n",
    "        \"professional\": 4,            # 4% professionals  \n",
    "        \"regular_interaction\": 3,     # 3% other regular interaction (40% - 33% - 4%)\n",
    "        \"some_experience\": 30,        # 30% some experience but not regular (70% - 40%)\n",
    "        \"no_experience\": 30          # 30% no direct experience (100% - 70%)\n",
    "    }\n",
    "    \n",
    "    # Knowledge level distribution\n",
    "    knowledge_level_distribution = {\n",
    "        \"none\": 6,     # 6%\n",
    "        \"little\": 65,  # 65%\n",
    "        \"a_lot\": 29    # 29%\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"age\": age_distribution,\n",
    "        \"gender\": gender_distribution,\n",
    "        \"location\": location_distribution,\n",
    "        \"autism_experience\": autism_experience_distribution,\n",
    "        \"knowledge_level\": knowledge_level_distribution\n",
    "    }\n",
    "\n",
    "# Create distributions\n",
    "distributions = create_demographic_distributions()\n",
    "print(\"Demographic distributions created successfully!\")\n",
    "print(f\"Age distribution: {distributions['age']}\")\n",
    "print(f\"Gender distribution: {distributions['gender']}\")\n",
    "print(f\"Location distribution: {distributions['location']}\")\n",
    "print(f\"Autism experience distribution: {distributions['autism_experience']}\")\n",
    "print(f\"Knowledge level distribution: {distributions['knowledge_level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fda8b-45ad-4168-992c-cbf52a3a23ca",
   "metadata": {},
   "source": [
    "#### 2.2 Profile Generation System\n",
    "##### Generate 100 different profiles across 3 dimensions: basic demographic information; autism-related experiences; autism-related knowledge\n",
    "##### First design the profiles with the most important dimension: autism-related experiences\n",
    "##### Then use the \"weighted_random_choice\" to incorporate other dimensions into the profile desig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49578e07-6a1f-4816-af25-01859bf81e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2.2: Profile Generation ===\n",
      "Profile generation completed!\n",
      "Total profiles generated: 100\n",
      "Autism experience verification: {'some_experience': 30, 'no_experience': 30, 'caregiver': 33, 'regular_interaction': 3, 'professional': 4}\n",
      "\n",
      "First 5 profiles:\n",
      "Profile 1: {'id': 1, 'autism_experience': 'some_experience', 'age_group': '30-35', 'gender': 'male', 'location': 'UK', 'knowledge_level': 'little'}\n",
      "Profile 2: {'id': 2, 'autism_experience': 'some_experience', 'age_group': '35-40', 'gender': 'female', 'location': 'Australia', 'knowledge_level': 'little'}\n",
      "Profile 3: {'id': 3, 'autism_experience': 'no_experience', 'age_group': '30-35', 'gender': 'female', 'location': 'US', 'knowledge_level': 'little'}\n",
      "Profile 4: {'id': 4, 'autism_experience': 'caregiver', 'age_group': '35-40', 'gender': 'male', 'location': 'US', 'knowledge_level': 'little'}\n",
      "Profile 5: {'id': 5, 'autism_experience': 'some_experience', 'age_group': '40-50', 'gender': 'female', 'location': 'US', 'knowledge_level': 'none'}\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Generate 100 individual profiles\n",
    "print(\"\\n=== Step 2.2: Profile Generation ===\")\n",
    "\n",
    "\"\"\"\n",
    "Here randomly select the specific dimensional information according to their distribution\n",
    "e.g. For female(51%) --> 51% of probability to select the gender of the specific profile to be female\n",
    "In this way, 100 profiles are generated\n",
    "\"\"\"\n",
    "def weighted_random_choice(distribution_dict: Dict[str, int]) -> str:\n",
    "    \"\"\"Generate weighted random choice based on distribution\"\"\"\n",
    "    choices = []\n",
    "    weights = []\n",
    "    for choice, weight in distribution_dict.items():\n",
    "        choices.append(choice)\n",
    "        weights.append(weight)\n",
    "    \n",
    "    return random.choices(choices, weights=weights, k=1)[0]\n",
    "\n",
    "def generate_responder_profiles(n: int = 100) -> List[Dict]:\n",
    "    \"\"\"Generate n responder profiles with realistic demographic combinations\"\"\"\n",
    "    profiles = []\n",
    "    \n",
    "    # First, create the exact distribution for autism experience (most constrained)\n",
    "    experience_types = []\n",
    "    for exp_type, count in distributions['autism_experience'].items():\n",
    "        experience_types.extend([exp_type] * count)\n",
    "    \n",
    "    # Shuffle to randomize order\n",
    "    random.shuffle(experience_types)\n",
    "    \n",
    "    # Generate profiles\n",
    "    for i, exp_type in enumerate(experience_types):\n",
    "        profile = {\n",
    "            'id': i + 1,\n",
    "            'autism_experience': exp_type,\n",
    "            'age_group': weighted_random_choice(distributions['age']),\n",
    "            'gender': weighted_random_choice(distributions['gender']),\n",
    "            'location': weighted_random_choice(distributions['location']),\n",
    "            'knowledge_level': weighted_random_choice(distributions['knowledge_level'])\n",
    "        }\n",
    "        profiles.append(profile)\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "# Generate 100 profiles\n",
    "responder_profiles = generate_responder_profiles(100)\n",
    "\n",
    "# Verify distributions\n",
    "print(\"Profile generation completed!\")\n",
    "print(f\"Total profiles generated: {len(responder_profiles)}\")\n",
    "\n",
    "# Verify distribution accuracy\n",
    "exp_counts = {}\n",
    "for profile in responder_profiles:\n",
    "    exp = profile['autism_experience']\n",
    "    exp_counts[exp] = exp_counts.get(exp, 0) + 1\n",
    "\n",
    "print(f\"Autism experience verification: {exp_counts}\")\n",
    "\n",
    "# Display first 5 profiles as examples\n",
    "print(\"\\nFirst 5 profiles:\")\n",
    "for i in range(5):\n",
    "    print(f\"Profile {i+1}: {responder_profiles[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd02a35-6943-47dd-ba20-6c88dbcca6e2",
   "metadata": {},
   "source": [
    "#### 2.3 Prompt Template Generation\n",
    "##### Combine the Amazon MTurk template with our designed 100 unique demographic profiles, get the unique 100 templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a7783d-e90c-408c-b9dc-9d61aa3fc845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2.3: Prompt Template System ===\n",
      "Prompt template system created successfully!\n",
      "\n",
      "Test prompt preview (first 500 characters):\n",
      "Introduction: Many individuals with autism visit online communities to ask questions about issues they are facing. We would like to know whether people outside of the specific community could provide answers for them. A question from a special autism community will be shown to you. Please read the question carefully, rate your confidence and provide your own answer to the question.\n",
      "Question Title: Dr Who, is he Autistic?\n",
      "PS. He is called The Doctor, not Dr Who. If you refer to him as Dr Who I can guarantee the following post will be a correction, simply because we all enjoy correcting people so much!\"lly and surprises people when he bumbles on about some kind of alien technology. This seemed especially clear with this weeks episode with James Corden. \n",
      "\n",
      "Tasks:\n",
      "BACKGROUND: You are a 30-35 year old male living in the United Kingdom. You have some limited experience with autism (through volunteering, brief encounters, or training), and you know a little about autism. \n",
      "When answering, draw from your specific background and experience level.\n",
      "\n",
      "Now, based on your background and experience:\n",
      "\n",
      "1. Rate your confidence in being able to answer this question.\n",
      "   (1) Not confident at all – (2) Slightly confident – \n",
      "   (3) Somewhat confident – (4) Very confident – \n",
      "   (5) Extremely confident\n",
      "\n",
      "2. Explain the reason for your confidence rating:\n",
      "\n",
      "3. Please answer the question, or explain why you cannot answer:\n",
      "\n",
      "IMPORTANT: Keep your answer (task 3) between 39-59 words. This should be a concise but helpful response, similar to typical online forum answers. Focus on providing direct, actionable advice rather than lengthy explanations.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Create prompt template system\n",
    "print(\"\\n=== Step 2.3: Prompt Template System ===\")\n",
    "\n",
    "\"\"\"\n",
    "Here this function convert a distribution information into a natural language expression to incorporate this information into the prompt description\n",
    "\"\"\"\n",
    "def create_persona_context(profile: Dict) -> str:\n",
    "    \"\"\"Generate persona context based on profile\"\"\"\n",
    "    \n",
    "    # Age description\n",
    "    age_ranges = {\n",
    "        \"25-30\": \"25-30\",\n",
    "        \"30-35\": \"30-35\", \n",
    "        \"35-40\": \"35-40\",\n",
    "        \"40-50\": \"40-50\"\n",
    "    }\n",
    "    age_desc = f\"You are a {age_ranges[profile['age_group']]} year old\"\n",
    "    \n",
    "    # Gender and location\n",
    "    gender_desc = profile['gender']\n",
    "    location_map = {\n",
    "        \"US\": \"the United States\",\n",
    "        \"Canada\": \"Canada\",\n",
    "        \"UK\": \"the United Kingdom\",\n",
    "        \"Australia\": \"Australia\", \n",
    "        \"New_Zealand\": \"New Zealand\",\n",
    "        \"Ireland\": \"Ireland\"\n",
    "    }\n",
    "    location_desc = f\"living in {location_map[profile['location']]}\"\n",
    "    \n",
    "    # Autism experience description\n",
    "    exp_descriptions = {\n",
    "        'caregiver': \"You are a caregiver (parent/spouse/sibling) of someone with autism\",\n",
    "        'professional': \"You work professionally with individuals with autism (teacher/therapist/social worker)\",\n",
    "        'regular_interaction': \"You regularly interact with someone with autism (friend/colleague/neighbor)\",\n",
    "        'some_experience': \"You have some limited experience with autism (through volunteering, brief encounters, or training)\",\n",
    "        'no_experience': \"You have no direct personal experience with autism\"\n",
    "    }\n",
    "    exp_desc = exp_descriptions[profile['autism_experience']]\n",
    "    \n",
    "    # Knowledge level description\n",
    "    knowledge_map = {\n",
    "        \"none\": \"nothing\",\n",
    "        \"little\": \"a little\",\n",
    "        \"a_lot\": \"a lot\"\n",
    "    }\n",
    "    knowledge_desc = f\"and you know {knowledge_map[profile['knowledge_level']]} about autism\"\n",
    "    \n",
    "    return f\"\"\"BACKGROUND: {age_desc} {gender_desc} {location_desc}. {exp_desc}, {knowledge_desc}. \n",
    "When answering, draw from your specific background and experience level.\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The average word length of the Amazon MTurk responese = 49.27\n",
    "We calculate a dynamic range of length to limit the length of AI-responses: 80%-120% of the average MTurk out-group responses\n",
    "\"\"\"\n",
    "def create_length_constraint(average_word_length: float, constraint_type: str = \"flexible\") -> Tuple[int, int, str]:\n",
    "    \"\"\"Create length constraint based on average word length\"\"\"\n",
    "    \n",
    "    if constraint_type == \"flexible\":\n",
    "        # Flexible constraint: ±20%\n",
    "        min_length = max(30, int(average_word_length * 0.8))\n",
    "        max_length = int(average_word_length * 1.2)\n",
    "        instruction = f\"Keep your answer (task 3) between {min_length}-{max_length} words.\"\n",
    "    \n",
    "    return min_length, max_length, instruction\n",
    "\n",
    "\"\"\"\n",
    "Incorporate each part to build a complet prompt: Introduction + Question title&body + Personal context + Task instruction \n",
    "\"\"\"\n",
    "def create_mturk_style_prompt(profile: Dict, question_title: str, question_body: str, \n",
    "                             average_word_length: float) -> str:\n",
    "    \"\"\"Create MTurk-style prompt with demographic background\"\"\"\n",
    "    \n",
    "    # Base introduction from original MTurk template\n",
    "    introduction = \"\"\"Introduction: Many individuals with autism visit online communities to ask questions about issues they are facing. We would like to know whether people outside of the specific community could provide answers for them. A question from a special autism community will be shown to you. Please read the question carefully, rate your confidence and provide your own answer to the question.\"\"\"\n",
    "    \n",
    "    # Question section\n",
    "    question_section = f\"\"\"\n",
    "Question Title: {question_title}\n",
    "Question Body: \"{question_body}\"\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate persona context\n",
    "    persona_context = create_persona_context(profile)\n",
    "    \n",
    "    # Create length constraint\n",
    "    min_length, max_length, length_instruction = create_length_constraint(average_word_length)\n",
    "    \n",
    "    # Tasks section with background integration\n",
    "    tasks = f\"\"\"\n",
    "Tasks:\n",
    "{persona_context}\n",
    "\n",
    "Now, based on your background and experience:\n",
    "\n",
    "1. Rate your confidence in being able to answer this question.\n",
    "   (1) Not confident at all – (2) Slightly confident – \n",
    "   (3) Somewhat confident – (4) Very confident – \n",
    "   (5) Extremely confident\n",
    "\n",
    "2. Explain the reason for your confidence rating:\n",
    "\n",
    "3. Please answer the question, or explain why you cannot answer:\n",
    "\n",
    "IMPORTANT: {length_instruction} This should be a concise but helpful response, similar to typical online forum answers. Focus on providing direct, actionable advice rather than lengthy explanations.\n",
    "\"\"\"\n",
    "    \n",
    "    return introduction + question_section + tasks\n",
    "\n",
    "# Test the prompt generation system\n",
    "test_profile = responder_profiles[0] #use the first generated profile\n",
    "test_question_title = questions_df.iloc[0]['Title'] #the title of the first question\n",
    "test_question_body = questions_df.iloc[0]['Body'] #the content of the first question\n",
    "\n",
    "test_prompt = create_mturk_style_prompt(\n",
    "    test_profile, \n",
    "    test_question_title, \n",
    "    test_question_body, \n",
    "    average_answer_word_length\n",
    ")\n",
    "\n",
    "print(\"Prompt template system created successfully!\")\n",
    "print(f\"\\nTest prompt preview (first 500 characters):\")\n",
    "print(test_prompt[:] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb57c8-67b0-4f30-863e-0eb9ea995d42",
   "metadata": {},
   "source": [
    "### 2.4 run Simulation of out-group responders\n",
    "#### File Management Module + Task Generation Module + Run Experiment & Result Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ca6435-5be7-4aca-91bd-2d3bc96002ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464b19b-ca2c-43e9-8b37-7ee62c8899d5",
   "metadata": {},
   "source": [
    "### 3. Run Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f94ca0-7995-48a5-b101-9817ebc7e371",
   "metadata": {},
   "source": [
    "#### 3.1 Run the complete simulation for all 400 answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f65d19-61fa-4907-a088-bfe6890b1d2d",
   "metadata": {},
   "source": [
    "##### The function to run the whole simulation experiment and call all dependent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc0b7a0-80fd-4de1-a4d6-0a0ef4b08762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_simulation(model_name: str, target_answers: int = 400, \n",
    "                        profiles: List[Dict] = None, questions_df: pd.DataFrame = None,\n",
    "                        average_word_length: float = None):\n",
    "    \"\"\"\n",
    "    Run simulation with configurable model and answer count\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): OpenAI model to use (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n",
    "        target_answers (int): Total number of answers to generate (default: 400)\n",
    "        profiles (List[Dict]): List of demographic profiles\n",
    "        questions_df (pd.DataFrame): DataFrame containing questions\n",
    "        average_word_length (float): Target word length for responses\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (successful_responses, json_path, csv_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING SIMULATION: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate answers per question based on target total\n",
    "    total_questions = len(questions_df)\n",
    "    answers_per_question = target_answers // total_questions\n",
    "    actual_total = answers_per_question * total_questions\n",
    "    \n",
    "    print(f\"📊 Simulation Configuration:\")\n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Target total answers: {target_answers}\")\n",
    "    print(f\"  Questions available: {total_questions}\")\n",
    "    print(f\"  Answers per question: {answers_per_question}\")\n",
    "    print(f\"  Actual total answers: {actual_total}\")\n",
    "    print(f\"  Profiles to use: {answers_per_question} per question (from {len(profiles)} available)\")\n",
    "    \n",
    "    # Create model-specific output directory\n",
    "    base_output_dir = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\"\n",
    "    model_output_dir = os.path.join(base_output_dir, model_name.replace(\".\", \"_\"))  # Replace dots for folder name\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    print(f\"📁 Model-specific output directory: {model_output_dir}\")\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    try:\n",
    "        client = verify_openai_access(\n",
    "            pathlib.Path(\"openai_organization.txt\"),\n",
    "            pathlib.Path(\"openai_api_key.txt\")\n",
    "        )\n",
    "        print(f\"✅ OpenAI client initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to initialize OpenAI client: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Configure model settings\n",
    "    model_settings = OpenAIModelSettings(\n",
    "        model=model_name,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.3,\n",
    "        n=1,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        stop=None,\n",
    "        params_descriptor=f\"autism-simulation-{model_name}\"\n",
    "    )\n",
    "    \n",
    "    # Generate balanced assignments\n",
    "    assignments = generate_balanced_profile_question_assignments_flexible(\n",
    "        profiles, questions_df, answers_per_question\n",
    "    )\n",
    "    \n",
    "    # Verify profile coverage\n",
    "    expected_profiles = min(len(profiles), actual_total)\n",
    "    coverage_verified = verify_profile_coverage_flexible(assignments, expected_profiles)\n",
    "    \n",
    "    if not coverage_verified:\n",
    "        print(f\"❌ Profile coverage verification failed!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Initialize output files with model-specific paths\n",
    "    json_path, csv_path = initialize_output_files_for_model(model_output_dir, model_name)\n",
    "    \n",
    "    print(f\"🚀 Starting simulation execution...\")\n",
    "    # Execute simulation\n",
    "    successful_responses = 0\n",
    "    failed_responses = 0\n",
    "    \n",
    "    with tqdm(total=len(assignments), desc=f\"Running {model_name}\", unit=\"response\") as pbar:\n",
    "        \n",
    "        for assignment in assignments:\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = create_mturk_style_prompt(\n",
    "                assignment['profile'], \n",
    "                assignment['question_title'], \n",
    "                assignment['question_body'], \n",
    "                average_word_length\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Call OpenAI API\n",
    "                response = call_openai_chat_api(prompt, model_settings, client)\n",
    "                \n",
    "                # Create result object\n",
    "                result = {\n",
    "                    'assignment_id': assignment['assignment_id'],\n",
    "                    'question_idx': assignment['question_idx'],\n",
    "                    'question_title': assignment['question_title'],\n",
    "                    'question_body': assignment['question_body'],\n",
    "                    'profile_id': assignment['profile_id'],\n",
    "                    'profile': assignment['profile'],\n",
    "                    'prompt': prompt,\n",
    "                    'response': response,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'model': model_name,\n",
    "                    'word_count': len(response.split()),\n",
    "                    'status': 'success'\n",
    "                }\n",
    "                \n",
    "                # Save immediately\n",
    "                append_result_to_files_robust(result, json_path, csv_path)\n",
    "                successful_responses += 1\n",
    "                                # Update progress\n",
    "                pbar.set_postfix({\n",
    "                    'Success': successful_responses,\n",
    "                    'Failed': failed_responses,\n",
    "                    'Rate': f\"{successful_responses/(successful_responses+failed_responses)*100:.1f}%\"\n",
    "                })\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_responses += 1\n",
    "                print(f\"\\n⚠️  Error in assignment {assignment['assignment_id']}: {e}\")\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Success': successful_responses,\n",
    "                    'Failed': failed_responses,\n",
    "                    'Rate': f\"{successful_responses/(successful_responses+failed_responses)*100:.1f}%\" if (successful_responses+failed_responses) > 0 else \"0%\"\n",
    "                })\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n📈 {model_name} Results:\")\n",
    "    print(f\"  ✅ Successful: {successful_responses}\")\n",
    "    print(f\"  ❌ Failed: {failed_responses}\")\n",
    "    print(f\"  📊 Success rate: {successful_responses/(successful_responses+failed_responses)*100:.1f}%\")\n",
    "    print(f\"  📁 Saved to: {model_output_dir}\")\n",
    "    \n",
    "    return successful_responses, json_path, csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8a520-93fd-4a17-ae90-44685c113cfd",
   "metadata": {},
   "source": [
    "#### According to required answer numbers and given question set, assign average number of simulated responders (with profiles) to each question + verify whether required answer number is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "108eca9c-75ef-4e10-87a1-e2dc109ffd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_balanced_profile_question_assignments_flexible(profiles: List[Dict], \n",
    "                                                          questions_df: pd.DataFrame, \n",
    "                                                          answers_per_question: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate balanced assignments with flexible answer count per question\n",
    "    \n",
    "    Args:\n",
    "        profiles (List[Dict]): Available profiles\n",
    "        questions_df (pd.DataFrame): Questions to answer\n",
    "        answers_per_question (int): How many answers per question\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: List of assignments\n",
    "    \"\"\"\n",
    "    \n",
    "    total_assignments = len(questions_df) * answers_per_question\n",
    "    total_profiles = len(profiles)\n",
    "    \n",
    "    print(f\"🔄 Generating flexible assignments:\")\n",
    "    print(f\"  Total assignments needed: {total_assignments}\")\n",
    "    print(f\"  Profiles available: {total_profiles}\")\n",
    "    \n",
    "    # If we need more assignments than profiles, repeat profiles\n",
    "    if total_assignments > total_profiles:\n",
    "        uses_per_profile = total_assignments // total_profiles\n",
    "        remaining = total_assignments % total_profiles\n",
    "        \n",
    "        expanded_profiles = []\n",
    "                # Each profile used equally\n",
    "        for _ in range(uses_per_profile):\n",
    "            expanded_profiles.extend(profiles.copy())\n",
    "        \n",
    "        # Add remaining profiles randomly\n",
    "        if remaining > 0:\n",
    "            extra_profiles = random.sample(profiles, remaining)\n",
    "            expanded_profiles.extend(extra_profiles)\n",
    "            \n",
    "        print(f\"  Each profile used ~{uses_per_profile} times\")\n",
    "\n",
    "    else:\n",
    "        # More profiles than needed, sample randomly\n",
    "        expanded_profiles = random.sample(profiles, total_assignments)\n",
    "        print(f\"  Using {total_assignments} profiles randomly selected\")\n",
    "    \n",
    "    # Shuffle for randomization\n",
    "    random.shuffle(expanded_profiles)\n",
    "    \n",
    "    # Assign to questions\n",
    "    assignments = []\n",
    "    profile_index = 0\n",
    "    \n",
    "    for question_idx, question_row in questions_df.iterrows():\n",
    "        for _ in range(answers_per_question):\n",
    "            profile = expanded_profiles[profile_index]\n",
    "            \n",
    "            assignment = {\n",
    "                'assignment_id': len(assignments) + 1,\n",
    "                'question_idx': question_idx,\n",
    "                'question_title': question_row['Title'],\n",
    "                'question_body': question_row['Body'],\n",
    "                'profile_id': profile['id'],\n",
    "                'profile': profile\n",
    "            }\n",
    "            assignments.append(assignment)\n",
    "            profile_index += 1\n",
    "    \n",
    "    print(f\"✅ Generated {len(assignments)} assignments\")\n",
    "    return assignments\n",
    "\n",
    "def verify_profile_coverage_flexible(assignments: List[Dict], expected_profiles: int) -> bool:\n",
    "    \"\"\"\n",
    "    Verify profile coverage for flexible assignment counts\n",
    "    \"\"\"\n",
    "    \n",
    "    profile_usage = {}\n",
    "    for assignment in assignments:\n",
    "        profile_id = assignment['profile_id']\n",
    "        profile_usage[profile_id] = profile_usage.get(profile_id, 0) + 1\n",
    "    \n",
    "    unique_profiles = len(profile_usage)\n",
    "    print(f\"👥 Profile coverage: {unique_profiles} unique profiles used\")\n",
    "    \n",
    "    if unique_profiles >= min(expected_profiles, 100):  # At most 100 profiles available\n",
    "        print(f\"✅ Adequate profile coverage\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"⚠️  Limited profile coverage\")\n",
    "        return True  # Still proceed, but note the limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9614a34-8a7c-4c1e-a71d-df8fc6a02632",
   "metadata": {},
   "source": [
    "#### Initialize output file + Write into json/csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fddecb0b-3843-4d56-a8bc-e663087b3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_output_files_for_model(model_output_dir: str, model_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Initialize output files in model-specific directory\n",
    "    \"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Model-specific file names\n",
    "    json_filename = f\"{model_name}_simulation_results_{timestamp}.json\"\n",
    "    csv_filename = f\"{model_name}_simulation_summary_{timestamp}.csv\"\n",
    "    \n",
    "    json_path = os.path.join(model_output_dir, json_filename)\n",
    "    csv_path = os.path.join(model_output_dir, csv_filename)\n",
    "    \n",
    "    # Initialize files\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump([], f)\n",
    "    \n",
    "    # MODIFIED: Added question_body to CSV headers\n",
    "    csv_headers = [\n",
    "        'assignment_id', 'profile_id', 'age_group', 'gender', 'location', \n",
    "        'autism_experience', 'knowledge_level', 'question_idx', 'question_title', \n",
    "        'question_body', 'response', 'timestamp', 'model', 'word_count', 'status'\n",
    "    ]\n",
    "    csv_df = pd.DataFrame(columns=csv_headers)\n",
    "    csv_df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    return json_path, csv_path\n",
    "\n",
    "\n",
    "def append_result_to_files_robust(result: Dict, json_path: str, csv_path: str, max_retries: int = 3):\n",
    "    \"\"\"\n",
    "    Robust file writing with retry mechanism\n",
    "    \"\"\"\n",
    "    \n",
    "    # JSON writing\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "        existing_data.append(result)\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(existing_data, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON write error: {e}\")\n",
    "    \n",
    "    # CSV writing with retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # MODIFIED: Added question_body to CSV row\n",
    "            csv_row = {\n",
    "                'assignment_id': result['assignment_id'],\n",
    "                'profile_id': result['profile_id'],\n",
    "                'age_group': result['profile']['age_group'],\n",
    "                'gender': result['profile']['gender'],\n",
    "                'location': result['profile']['location'],\n",
    "                'autism_experience': result['profile']['autism_experience'],\n",
    "                'knowledge_level': result['profile']['knowledge_level'],\n",
    "                'question_idx': result['question_idx'],\n",
    "                'question_title': result['question_title'],\n",
    "                'question_body': result['question_body'],  # ADDED: Question body column\n",
    "                'response': result['response'],\n",
    "                'timestamp': result['timestamp'],\n",
    "                'model': result['model'],\n",
    "                'word_count': len(result['response'].split()),\n",
    "                'status': result.get('status', 'success')\n",
    "            }\n",
    "            \n",
    "            csv_row_df = pd.DataFrame([csv_row])\n",
    "            csv_row_df.to_csv(csv_path, mode='a', header=False, index=False, encoding='utf-8')\n",
    "            break\n",
    "            \n",
    "        except PermissionError:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"CSV write error: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebcda6-a123-42ce-a014-5c2078353f64",
   "metadata": {},
   "source": [
    "#### Simulate results for different models through iteration of different models and save them in differnet folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a68309b4-8852-43a9-b646-1b45b4033d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MULTI-MODEL BATCH TESTING\n",
    "# ===================================================================\n",
    "\n",
    "def run_multi_model_comparison(target_answers: int = 100):\n",
    "    \"\"\"\n",
    "    Run simulation across multiple models for comparison\n",
    "    \n",
    "    Args:\n",
    "        target_answers (int): Number of answers to generate per model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define models to test\n",
    "    models_to_test = [\n",
    "        \"gpt-4.1-mini\",\n",
    "        # \"gpt-4\", \n",
    "        # \"gpt-4-turbo\",  # Note: corrected from \"gpt-4.1mini\" which doesn't exist\n",
    "        \"gpt-3.5-turbo\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MULTI-MODEL COMPARISON STUDY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"🎯 Target answers per model: {target_answers}\")\n",
    "    print(f\"🤖 Models to test: {', '.join(models_to_test)}\")\n",
    "    print(f\"📊 Total answers to generate: {len(models_to_test) * target_answers}\")\n",
    "    \n",
    "    # Set random seed for consistency across models\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Storage for results comparison\n",
    "    model_results = {}\n",
    "    \n",
    "    # Run simulation for each model\n",
    "    for i, model_name in enumerate(models_to_test, 1):\n",
    "        \n",
    "        print(f\"\\n🚀 STARTING MODEL {i}/{len(models_to_test)}: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Reset random seed for each model to ensure identical assignments\n",
    "            random.seed(42)\n",
    "            \n",
    "            # Run simulation\n",
    "            results = run_model_simulation(\n",
    "                model_name=model_name,\n",
    "                target_answers=target_answers,\n",
    "                profiles=responder_profiles,\n",
    "                questions_df=questions_df,\n",
    "                average_word_length=average_answer_word_length\n",
    "            )\n",
    "            \n",
    "            successful_responses, json_path, csv_path = results\n",
    "            \n",
    "            # Store results\n",
    "            model_results[model_name] = {\n",
    "                'successful_responses': successful_responses,\n",
    "                'json_path': json_path,\n",
    "                'csv_path': csv_path,\n",
    "                'success_rate': successful_responses / target_answers * 100 if successful_responses else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {model_name} completed: {successful_responses}/{target_answers} responses\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {model_name} failed: {e}\")\n",
    "            model_results[model_name] = {\n",
    "                'successful_responses': 0,\n",
    "                'json_path': None,\n",
    "                'csv_path': None,\n",
    "                'success_rate': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "        # Add delay between models to avoid rate limiting\n",
    "        if i < len(models_to_test):\n",
    "            print(f\"⏳ Waiting 30 seconds before next model...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    # Display comparison summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MULTI-MODEL COMPARISON RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"📊 Results Summary:\")\n",
    "    print(f\"{'Model':<15} {'Success':<8} {'Rate':<8} {'Status':<10}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    for model_name, result in model_results.items():\n",
    "        success = result['successful_responses']\n",
    "        rate = f\"{result['success_rate']:.1f}%\"\n",
    "        status = \"✅ OK\" if success > 0 else \"❌ FAIL\"\n",
    "        \n",
    "        print(f\"{model_name:<15} {success:<8} {rate:<8} {status:<10}\")\n",
    "    \n",
    "    # Save comparison summary\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            'model': model_name,\n",
    "            'successful_responses': result['successful_responses'],\n",
    "            'target_responses': target_answers,\n",
    "            'success_rate': result['success_rate'],\n",
    "            'json_path': result.get('json_path', ''),\n",
    "            'csv_path': result.get('csv_path', ''),\n",
    "            'error': result.get('error', '')\n",
    "        }\n",
    "        for model_name, result in model_results.items()\n",
    "    ])\n",
    "    \n",
    "    # Save comparison to base directory\n",
    "    base_dir = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\"\n",
    "    comparison_path = os.path.join(base_dir, f\"model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    \n",
    "    print(f\"\\n📁 Comparison summary saved: {comparison_path}\")\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da7c4e-984a-4338-afb6-b90d534f6d30",
   "metadata": {},
   "source": [
    "#### Codes to execute the real experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2d2a1f3-8632-4f02-8306-f1ee1d6d94ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 MULTI-MODEL AUTISM SIMULATION EXPERIMENT\n",
      "============================================================\n",
      "📋 Pre-execution checklist:\n",
      "  ✅ responder_profiles: Available\n",
      "  ✅ questions_df: Available\n",
      "  ✅ average_answer_word_length: Available\n",
      "✅ All prerequisites met\n",
      "\n",
      "🎯 Choose test scale:\n",
      "  1. Quick test: 100 answers (5 per question)\n",
      "  2. Medium test: 200 answers (10 per question)\n",
      "  3. Full test: 400 answers (20 per question)\n",
      "\n",
      "🚀 Starting multi-model comparison with 100 answers per model\n",
      "\n",
      "================================================================================\n",
      "MULTI-MODEL COMPARISON STUDY\n",
      "================================================================================\n",
      "🎯 Target answers per model: 100\n",
      "🤖 Models to test: gpt-4.1-mini, gpt-3.5-turbo\n",
      "📊 Total answers to generate: 200\n",
      "\n",
      "🚀 STARTING MODEL 1/2: gpt-4.1-mini\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RUNNING SIMULATION: GPT-4.1-MINI\n",
      "============================================================\n",
      "📊 Simulation Configuration:\n",
      "  Model: gpt-4.1-mini\n",
      "  Target total answers: 100\n",
      "  Questions available: 20\n",
      "  Answers per question: 5\n",
      "  Actual total answers: 100\n",
      "  Profiles to use: 5 per question (from 100 available)\n",
      "📁 Model-specific output directory: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\gpt-4_1-mini\n",
      "✅ OpenAI client initialized\n",
      "🔄 Generating flexible assignments:\n",
      "  Total assignments needed: 100\n",
      "  Profiles available: 100\n",
      "  Using 100 profiles randomly selected\n",
      "✅ Generated 100 assignments\n",
      "👥 Profile coverage: 100 unique profiles used\n",
      "✅ Adequate profile coverage\n",
      "🚀 Starting simulation execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gpt-4.1-mini: 100%|████████████████| 100/100 [04:54<00:00,  2.95s/response, Success=100, Failed=0, Rate=100.0%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 gpt-4.1-mini Results:\n",
      "  ✅ Successful: 100\n",
      "  ❌ Failed: 0\n",
      "  📊 Success rate: 100.0%\n",
      "  📁 Saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\gpt-4_1-mini\n",
      "✅ gpt-4.1-mini completed: 100/100 responses\n",
      "⏳ Waiting 30 seconds before next model...\n",
      "\n",
      "🚀 STARTING MODEL 2/2: gpt-3.5-turbo\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RUNNING SIMULATION: GPT-3.5-TURBO\n",
      "============================================================\n",
      "📊 Simulation Configuration:\n",
      "  Model: gpt-3.5-turbo\n",
      "  Target total answers: 100\n",
      "  Questions available: 20\n",
      "  Answers per question: 5\n",
      "  Actual total answers: 100\n",
      "  Profiles to use: 5 per question (from 100 available)\n",
      "📁 Model-specific output directory: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\gpt-3_5-turbo\n",
      "✅ OpenAI client initialized\n",
      "🔄 Generating flexible assignments:\n",
      "  Total assignments needed: 100\n",
      "  Profiles available: 100\n",
      "  Using 100 profiles randomly selected\n",
      "✅ Generated 100 assignments\n",
      "👥 Profile coverage: 100 unique profiles used\n",
      "✅ Adequate profile coverage\n",
      "🚀 Starting simulation execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gpt-3.5-turbo: 100%|███████████████| 100/100 [09:35<00:00,  5.76s/response, Success=100, Failed=0, Rate=100.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 gpt-3.5-turbo Results:\n",
      "  ✅ Successful: 100\n",
      "  ❌ Failed: 0\n",
      "  📊 Success rate: 100.0%\n",
      "  📁 Saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\gpt-3_5-turbo\n",
      "✅ gpt-3.5-turbo completed: 100/100 responses\n",
      "\n",
      "================================================================================\n",
      "MULTI-MODEL COMPARISON RESULTS\n",
      "================================================================================\n",
      "📊 Results Summary:\n",
      "Model           Success  Rate     Status    \n",
      "--------------------------------------------------\n",
      "gpt-4.1-mini    100      100.0%   ✅ OK      \n",
      "gpt-3.5-turbo   100      100.0%   ✅ OK      \n",
      "\n",
      "📁 Comparison summary saved: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\model_comparison_20250722_151146.csv\n",
      "\n",
      "🎉 Multi-model comparison completed!\n",
      "Check the individual model folders for detailed results:\n",
      "📁 D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\n",
      "  📂 gpt-4.1-mini/\n",
      "  📂 gpt-3.5-turbo/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# EXECUTION CODE\n",
    "# ===================================================================\n",
    "\n",
    "print(\"🔧 MULTI-MODEL AUTISM SIMULATION EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify prerequisites\n",
    "print(\"📋 Pre-execution checklist:\")\n",
    "required_vars = ['responder_profiles', 'questions_df', 'average_answer_word_length']\n",
    "all_ready = True\n",
    "\n",
    "for var in required_vars:\n",
    "    if var in globals():\n",
    "        print(f\"  ✅ {var}: Available\")\n",
    "    else:\n",
    "        print(f\"  ❌ {var}: Missing\")\n",
    "        all_ready = False\n",
    "\n",
    "if not all_ready:\n",
    "    print(\"❌ Please run previous steps to generate required variables\")\n",
    "else:\n",
    "    print(\"✅ All prerequisites met\")\n",
    "    \n",
    "    # Choose test scale\n",
    "    print(f\"\\n🎯 Choose test scale:\")\n",
    "    print(f\"  1. Quick test: 100 answers (5 per question)\")\n",
    "    print(f\"  2. Medium test: 200 answers (10 per question)\")\n",
    "    print(f\"  3. Full test: 400 answers (20 per question)\")\n",
    "    \n",
    "    # For demonstration, let's use medium test\n",
    "    test_scale = 100  # You can change this\n",
    "    \n",
    "    print(f\"\\n🚀 Starting multi-model comparison with {test_scale} answers per model\")\n",
    "    \n",
    "    # Run the comparison\n",
    "    try:\n",
    "        comparison_results = run_multi_model_comparison(target_answers=test_scale)\n",
    "        \n",
    "        print(f\"\\n🎉 Multi-model comparison completed!\")\n",
    "        print(f\"Check the individual model folders for detailed results:\")\n",
    "        \n",
    "        base_dir = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\"\n",
    "        print(f\"📁 {base_dir}\")\n",
    "        \n",
    "        for model_name in [\"gpt-4.1-mini\", \"gpt-3.5-turbo\"]:\n",
    "            model_dir = os.path.join(base_dir, model_name.replace(\".\", \"_\"))\n",
    "            if os.path.exists(model_dir):\n",
    "                print(f\"  📂 {model_name}/\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe2d15-6663-453b-b116-2908bd24e670",
   "metadata": {},
   "source": [
    "### 4. Rater Simulation\n",
    "#### 4.1 Question & Answer Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd05dc3-4f3d-4ebf-b79f-2497e34d8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import hashlib\n",
    "\n",
    "# Suppress NumPy warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in divide')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='divide by zero encountered')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7838c-bf21-49eb-b288-30535cb1346e",
   "metadata": {},
   "source": [
    "### 4.1.1 Rater Profile Creation:\n",
    "#### Researchers(n=2) + Individuals with Autism(n=6) + Autism Experts(n=11)\n",
    "#### Create bias and consistency variables for each rater, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c384094-4055-422f-a703-532ac0c7935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_rater_profiles():\n",
    "    \"\"\"\n",
    "    Create three groups of rater profiles based on original paper\n",
    "    \"\"\"\n",
    "    \n",
    "    # Researcher group (2 people) - evaluate all 5 criteria\n",
    "    researchers = [\n",
    "        {\n",
    "            'id': 'R1',\n",
    "            'group': 'Researchers',\n",
    "            'expertise': 'autism community research',\n",
    "            'description': 'a researcher with expertise in autism community research who participated in the in-group question classification process',\n",
    "            'evaluation_criteria': ['Directness', 'Additional_Information', 'Informational_Support', 'Emotional_Support', 'Helpfulness'],\n",
    "            'bias': -0.1,  # Slightly conservative\n",
    "            'consistency': 0.85\n",
    "        },\n",
    "        {\n",
    "            'id': 'R2', \n",
    "            'group': 'Researchers',\n",
    "            'expertise': 'autism community research',\n",
    "            'description': 'a researcher with expertise in autism community research who participated in the in-group question classification process',\n",
    "            'evaluation_criteria': ['Directness', 'Additional_Information', 'Informational_Support', 'Emotional_Support', 'Helpfulness'],\n",
    "            'bias': 0.05,  # Slightly lenient\n",
    "            'consistency': 0.88\n",
    "        }\n",
    "    ]\n",
    "    # Individuals with autism group (6 people) - only evaluate helpfulness\n",
    "    autism_individuals = [\n",
    "        {\n",
    "            'id': f'A{i}',\n",
    "            'group': 'Individuals_with_autism',\n",
    "            'expertise': 'lived experience with autism',\n",
    "            'description': 'an individual who self-identified as having high-functioning autism and understands the challenges from personal experience',\n",
    "            'evaluation_criteria': ['Helpfulness'],\n",
    "            'bias': (i-3.5) * 0.05,  # Range from -0.125 to +0.125\n",
    "            'consistency': 0.75 + (i % 3) * 0.05  # 0.75 to 0.85\n",
    "        }\n",
    "        for i in range(1, 7)\n",
    "    ]\n",
    "    \n",
    "    # Autism experts group (11 people) - only evaluate helpfulness\n",
    "    expert_roles = [\n",
    "        'vocational and transitional specialist', 'vocational and transitional specialist',\n",
    "        'vocational and transitional specialist', 'vocational and transitional specialist',\n",
    "        'special education teacher', 'special education teacher',\n",
    "        'job developer', 'job developer', 'job developer',\n",
    "        'behavior analyst', 'mother of a son with autism'\n",
    "    ]\n",
    "    autism_experts = [\n",
    "        {\n",
    "            'id': f'E{i}',\n",
    "            'group': 'Autism_experts',\n",
    "            'expertise': expert_roles[i-1],\n",
    "            'description': f'a {expert_roles[i-1]} recruited at an official meeting for designing workplace transition plans for students with autism',\n",
    "            'evaluation_criteria': ['Helpfulness'],\n",
    "            'bias': (i-6) * 0.02,  # Range from -0.1 to +0.1\n",
    "            'consistency': 0.8 + (i % 4) * 0.025  # 0.8 to 0.875\n",
    "        }\n",
    "        for i in range(1, 12)\n",
    "    ]\n",
    "    \n",
    "    all_raters = researchers + autism_individuals + autism_experts\n",
    "    \n",
    "    return all_raters, researchers, autism_individuals, autism_experts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3689cd-6df5-4a56-86b5-cd99fb073772",
   "metadata": {},
   "source": [
    "### 4.1.2 Enhanced LLM Prompt Template\n",
    "#### Give the criteria of the 5 measures of responses to those in-group questions\n",
    "#### \"Enhanced\" means these prompts include more clarifications/exmaples of each criteria apart from simple evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956b0079-9735-4048-83b5-1353aca6b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_researcher_evaluation_prompt(rater_profile: Dict, question: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced researcher prompt with explicit calibration examples\n",
    "    \"\"\"\n",
    "    \n",
    "    calibration_examples = \"\"\"\n",
    "CALIBRATION EXAMPLES for consistent rating:\n",
    "\n",
    "Directness Examples:\n",
    "- DIRECT (1): \"Yes, you should...\" \"The answer is...\" \"I recommend...\" \"Here's what to do...\"\n",
    "- INDIRECT (0): \"Many people feel...\" \"It depends...\" \"You might consider...\" without clear guidance\n",
    "\n",
    "Additional Information Examples:  \n",
    "- HAS EXTRA INFO (1): Provides context, background, related tips, or broader perspective\n",
    "- NO EXTRA INFO (0): Only addresses the specific question asked, nothing more\n",
    "\n",
    "Informational Support Examples:\n",
    "- PROVIDES SUPPORT (1): Gives specific advice, concrete steps, actionable knowledge, or practical guidance\n",
    "- NO SUPPORT (0): Only acknowledges feelings without actionable guidance\n",
    "\n",
    "Emotional Support Examples:\n",
    "- HAS EMOTIONAL SUPPORT (1): \"I understand,\" \"You're not alone,\" encouragement, validation, empathy\n",
    "- NO EMOTIONAL SUPPORT (0): Pure factual response without empathy or emotional connection\n",
    "\n",
    "Helpfulness Scale (be consistent with these anchors):\n",
    "- 5 = EXCELLENT: Comprehensive, actionable, emotionally supportive, directly addresses question\n",
    "- 4 = VERY GOOD: Good advice with minor gaps, quite helpful overall\n",
    "- 3 = GOOD: Adequate response, addresses question moderately well\n",
    "- 2 = FAIR: Somewhat helpful but lacks depth or has notable issues\n",
    "- 1 = POOR: Not helpful, off-topic, or potentially problematic\n",
    "\"\"\"    \n",
    "    prompt = f\"\"\"You are researcher {rater_profile['id']} with expertise in {rater_profile['expertise']}. You are {rater_profile['description']}.\n",
    "\n",
    "{calibration_examples}\n",
    "\n",
    "CRITICAL: Use these calibration examples as your EXACT reference point. Rate INDEPENDENTLY but consistently with these precise standards.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Response to evaluate: \"{answer}\"\n",
    "\n",
    "Rate using the exact criteria above. Be precise and consistent. Provide ONLY numbers, one per line:\n",
    "\n",
    "Directness (0/1):\n",
    "Additional Information (0/1):\n",
    "Informational Support (0/1):\n",
    "Emotional Support (0/1):\n",
    "Helpfulness (1-5):\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f98275-c9d8-4b10-878e-0d1f92639cda",
   "metadata": {},
   "source": [
    "#### Create particular perspectives for individuals with autism, specify criteria from their perspective\n",
    "#### Example: 5 = most helpful to autism people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0df0372-6488-4533-bd15-b37082f03113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual_evaluation_prompt(rater_profile: Dict, question: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced individual prompt with personal perspective calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    calibration_examples = \"\"\"\n",
    "HELPFULNESS CALIBRATION from personal autism perspective:\n",
    "\n",
    "5 = EXCELLENT: Would definitely help me or someone I know with autism - practical, understanding, actionable\n",
    "4 = VERY GOOD: Quite helpful, addresses autism-specific needs well, mostly practical\n",
    "3 = GOOD: Moderately helpful, generally appropriate, somewhat useful\n",
    "2 = FAIR: Somewhat helpful but missing important autism-specific aspects\n",
    "1 = POOR: Not helpful for autism community or potentially inappropriate\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are {rater_profile['id']}, an individual with high-functioning autism. You are {rater_profile['description']}.\n",
    "\n",
    "{calibration_examples}\n",
    "\n",
    "As someone with lived autism experience, rate how helpful this response would be to you or others in the autism community.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Response to evaluate: \"{answer}\"\n",
    "\n",
    "Consider: Does this truly help someone with autism in practical, real-world terms?\n",
    "\n",
    "Use the calibration scale above. Be consistent and INDEPENDENT.\n",
    "\n",
    "Helpfulness (1-5):\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829011f2-32bb-4b11-9373-2a6b09cea463",
   "metadata": {},
   "source": [
    "#### Create particular perspectives for autism experts, specify criteria from professional perspectives\n",
    "#### Example: 5 = professional and appropriate, giving evidence and specific about autism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fdc84d-a978-4a33-bf2c-317e04ac8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expert_evaluation_prompt(rater_profile: Dict, question: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced expert prompt with professional standards calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    calibration_examples = \"\"\"\n",
    "PROFESSIONAL HELPFULNESS CALIBRATION for autism support:\n",
    "\n",
    "5 = EXCELLENT: Evidence-based, professionally appropriate, autism-specific, safe, comprehensive\n",
    "4 = VERY GOOD: Professionally sound with minor limitations, generally evidence-based\n",
    "3 = GOOD: Generally appropriate, meets basic professional standards, adequate\n",
    "2 = FAIR: Somewhat helpful but lacks professional depth or has concerns\n",
    "1 = POOR: Unprofessional, inappropriate, potentially harmful, or inadequate\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are expert {rater_profile['id']}, a {rater_profile['expertise']}. You are {rater_profile['description']}.\n",
    "\n",
    "{calibration_examples}\n",
    "\n",
    "From your professional perspective working with individuals with autism, evaluate this response.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Response to evaluate: \"{answer}\"\n",
    "\n",
    "Rate based on professional appropriateness, evidence-based practice, and suitability for autism community.\n",
    "\n",
    "Use the calibration scale above. Be consistent and INDEPENDENT as a {rater_profile['expertise']}.\n",
    "\n",
    "Helpfulness (1-5):\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b8cad-aab8-4fc9-8f8a-7369c4f01603",
   "metadata": {},
   "source": [
    "### 4.1.3 Enhanced Parameter Generation\n",
    "#### Slightly adjust different parameters for different rater groups to create variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e5b61e-86f7-49de-86ea-fbb33c307044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enhanced_rater_parameters(rater_id: str, question_text: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced rater-specific parameters for optimal reliability balance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create stable hash for this rater\n",
    "    rater_hash = int(hashlib.md5(rater_id.encode()).hexdigest()[:8], 16) % 1000\n",
    "    \n",
    "    # Create question-specific slight adjustment for realistic variance\n",
    "    question_hash = int(hashlib.md5(question_text.encode()).hexdigest()[:8], 16) % 100 if question_text else 50\n",
    "    question_adjustment = (question_hash - 50) / 2000  # -0.025 to +0.025\n",
    "    \n",
    "    # Base parameters for HIGH reliability (much lower temperature)\n",
    "    base_params = {\n",
    "        'temperature': 1.0,  # Much lower for consistency\n",
    "        # 'top_p': 0.85,        # More focused\n",
    "        'presence_penalty': 0.0,\n",
    "        'frequency_penalty': 0.0,\n",
    "    }\n",
    "\n",
    "    # TEMPERATURE CHANGE    \n",
    "    # Rater-group specific minimal adjustments for realistic variance\n",
    "    if rater_id.startswith('R'):  # Researchers - highest consistency\n",
    "        base_params['temperature'] = 0.5 + (rater_hash % 3) * 0.005  # 0.05-0.06\n",
    "        # base_params['top_p'] = 0.88 + (rater_hash % 2) * 0.01  # 0.88-0.89\n",
    "        \n",
    "    elif rater_id.startswith('A'):  # Individuals - slight more variance\n",
    "        base_params['temperature'] = 0.5 + (rater_hash % 5) * 0.005  # 0.05-0.07\n",
    "        # base_params['top_p'] = 0.85 + (rater_hash % 3) * 0.01  # 0.85-0.87\n",
    "        \n",
    "    elif rater_id.startswith('E'):  # Experts - professional consistency\n",
    "        base_params['temperature'] = 0.5 + (rater_hash % 4) * 0.005  # 0.05-0.065\n",
    "        # base_params['top_p'] = 0.86 + (rater_hash % 3) * 0.01  # 0.86-0.88\n",
    "    \n",
    "    # Add tiny question-specific variance for realism\n",
    "    base_params['temperature'] += abs(question_adjustment)\n",
    "    \n",
    "    return base_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f759ae-1ebe-4ea0-9536-b6ba81e886fe",
   "metadata": {},
   "source": [
    "#### This part is usually not used, used only if LLM call fails (ignore the following 2 functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1f0a6c-8a02-49bb-98ea-36a62c0c9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_answer_quality(question: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Simulate underlying answer quality to create correlated ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simple heuristics for answer quality\n",
    "    word_count = len(answer.split())\n",
    "    \n",
    "    # Quality indicators\n",
    "    quality_score = 0.5  # baseline\n",
    "    \n",
    "    # Length-based quality (reasonable length is better)\n",
    "    if 20 <= word_count <= 150:\n",
    "        quality_score += 0.2\n",
    "    elif word_count < 10:\n",
    "        quality_score -= 0.2\n",
    "    \n",
    "    # Content-based indicators\n",
    "    helpful_words = ['recommend', 'suggest', 'try', 'help', 'support', 'understand', 'consider']\n",
    "    supportive_words = ['feel', 'understand', 'experience', 'know', 'been there']\n",
    "    \n",
    "    quality_score += min(0.2, len([w for w in helpful_words if w in answer.lower()]) * 0.05)\n",
    "    quality_score += min(0.1, len([w for w in supportive_words if w in answer.lower()]) * 0.02)\n",
    "    \n",
    "    # Add question-answer relevance (simple word overlap)\n",
    "    question_words = set(question.lower().split())\n",
    "    answer_words = set(answer.lower().split())\n",
    "    overlap = len(question_words.intersection(answer_words))\n",
    "    quality_score += min(0.1, overlap * 0.01)\n",
    "    \n",
    "    return max(0.1, min(0.9, quality_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4829eec-0d60-4813-8e93-58b9d5be1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlated_ratings(rater_profile: Dict, answer_quality: float, question: str, answer: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate correlated ratings based on answer quality and rater characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get rater characteristics\n",
    "    rater_bias = rater_profile.get('bias', 0.0)\n",
    "    rater_consistency = rater_profile.get('consistency', 0.8)\n",
    "    \n",
    "    # Random seed for this specific combination (consistent across runs)\n",
    "    seed_str = f\"{rater_profile['id']}_{hash(question)}_{hash(answer)}\"\n",
    "    np.random.seed(abs(hash(seed_str)) % 2147483647)\n",
    "    \n",
    "    if rater_profile['group'] == 'Researchers':\n",
    "        # Generate correlated binary ratings\n",
    "        base_threshold = 0.5 + rater_bias\n",
    "        noise_level = 0.15 * (1 - rater_consistency)\n",
    "        \n",
    "        directness = 1 if (answer_quality + np.random.normal(0, noise_level)) > base_threshold else 0\n",
    "        additional_info = 1 if (answer_quality + np.random.normal(0, noise_level)) > (base_threshold + 0.1) else 0\n",
    "        info_support = 1 if (answer_quality + np.random.normal(0, noise_level)) > (base_threshold - 0.1) else 0\n",
    "        emotional_support = 1 if (answer_quality + np.random.normal(0, noise_level)) > (base_threshold + 0.2) else 0\n",
    "        \n",
    "        # Correlated helpfulness rating\n",
    "        helpfulness_raw = answer_quality * 3.5 + 1.5 + rater_bias + np.random.normal(0, 0.3 * (1-rater_consistency))\n",
    "        helpfulness = max(1, min(5, round(helpfulness_raw)))\n",
    "        \n",
    "        return {\n",
    "            'Directness': directness,\n",
    "            'Additional_Information': additional_info,\n",
    "            'Informational_Support': info_support,\n",
    "            'Emotional_Support': emotional_support,\n",
    "            'Helpfulness': helpfulness\n",
    "        }\n",
    "    else:\n",
    "        # For individuals and experts, only helpfulness\n",
    "        helpfulness_raw = answer_quality * 3.5 + 1.5 + rater_bias + np.random.normal(0, 0.25 * (1-rater_consistency))\n",
    "        helpfulness = max(1, min(5, round(helpfulness_raw)))\n",
    "        \n",
    "        return {'Helpfulness': helpfulness}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70244ad9-a1f6-40b5-ba3f-45d1e36af989",
   "metadata": {},
   "source": [
    "#### Try to call LLM to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90656443-9ec8-4830-ba3d-76875e87d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_evaluation_llm(prompt: str, model_name: str = \"gpt-4o-mini\", rater_id: str = None, question: str = \"\", answer: str = \"\") -> str:\n",
    "   \"\"\"\n",
    "   Call LLM for evaluation with enhanced parameters and fallback simulation\n",
    "   \"\"\"\n",
    "   \n",
    "   try:\n",
    "       # Initialize OpenAI client\n",
    "       client = verify_openai_access(\n",
    "           pathlib.Path(\"openai_organization.txt\"),\n",
    "           pathlib.Path(\"openai_api_key.txt\")\n",
    "       )\n",
    "       \n",
    "       # Get enhanced rater-specific parameters\n",
    "       rater_params = get_enhanced_rater_parameters(rater_id, question)\n",
    "       \n",
    "       # Configure model settings - remove unsupported top_p parameter\n",
    "       model_settings = OpenAIModelSettings(\n",
    "           model=model_name,\n",
    "           max_tokens=50,  # Short output for focused scoring\n",
    "           temperature=rater_params['temperature'],\n",
    "           n=1,\n",
    "           presence_penalty=rater_params['presence_penalty'],\n",
    "           frequency_penalty=rater_params['frequency_penalty'],\n",
    "           stop=None,\n",
    "           params_descriptor=f\"rating-evaluation-{rater_id}\"\n",
    "       )\n",
    "       \n",
    "       # Add strong independence guidance\n",
    "       independence_instruction = f\"\"\"\n",
    "CRITICAL REMINDER: You are {rater_id} evaluating INDEPENDENTLY. Use your calibration standards consistently. Do not assume what other raters think. Rate based solely on the criteria provided.\n",
    "\"\"\"\n",
    "       \n",
    "       enhanced_prompt = prompt + independence_instruction\n",
    "       \n",
    "       # Call OpenAI API\n",
    "       response_dict = call_openai_chat_api(enhanced_prompt, model_settings, client)\n",
    "       \n",
    "       # Extract text content\n",
    "       response_text = response_dict['output']['choices'][0]['message']['content']\n",
    "       return response_text\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"LLM call failed for rater {rater_id}, using enhanced simulation: {e}\")\n",
    "       \n",
    "       # Use enhanced simulation as fallback\n",
    "       all_raters, researchers, autism_individuals, autism_experts = create_all_rater_profiles()\n",
    "       \n",
    "       # Find rater profile\n",
    "       rater_profile = None\n",
    "       for rater in all_raters:\n",
    "           if rater['id'] == rater_id:\n",
    "               rater_profile = rater\n",
    "               break\n",
    "       \n",
    "       if rater_profile:\n",
    "           # Generate quality-related ratings\n",
    "           answer_quality = simulate_answer_quality(question, answer)\n",
    "           ratings = generate_correlated_ratings(rater_profile, answer_quality, question, answer)\n",
    "           \n",
    "           if rater_profile['group'] == 'Researchers':\n",
    "               return f\"{ratings['Directness']}\\n{ratings['Additional_Information']}\\n{ratings['Informational_Support']}\\n{ratings['Emotional_Support']}\\n{ratings['Helpfulness']}\"\n",
    "           else:\n",
    "               return str(ratings['Helpfulness'])\n",
    "       else:\n",
    "           # Final fallback\n",
    "           import random\n",
    "           random.seed(hash(rater_id + question + answer))\n",
    "           if \"R\" in str(rater_id):\n",
    "               base_quality = random.choice([0, 1])\n",
    "               return f\"{base_quality}\\n{random.randint(0,1)}\\n{base_quality}\\n{random.randint(0,1)}\\n{random.randint(2,4)}\"\n",
    "           else:\n",
    "               return str(random.randint(2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb08e0-d359-4291-9c0c-58f8639e4001",
   "metadata": {},
   "source": [
    "#### Parse the rating results\n",
    "#### -Process the LLN response and extract the rating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c315af60-1a42-4349-b4c1-e7e1b3389ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_researcher_response(response_text: str, rater_id: str = None, question: str = \"\", answer: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Parse researcher LLM response with enhanced fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    if not response_text:\n",
    "        # Use quality-based simulation as fallback\n",
    "        answer_quality = simulate_answer_quality(question, answer)\n",
    "        all_raters, researchers, _, _ = create_all_rater_profiles()\n",
    "        rater_profile = next((r for r in researchers if r['id'] == rater_id), researchers[0])\n",
    "        return generate_correlated_ratings(rater_profile, answer_quality, question, answer)\n",
    "    \n",
    "    try:\n",
    "        lines = [line.strip() for line in response_text.strip().split('\\n') if line.strip()]\n",
    "        \n",
    "        # Extract numbers\n",
    "        import re\n",
    "        numbers = []\n",
    "        for line in lines:\n",
    "            found_numbers = re.findall(r'\\d+', line)\n",
    "            if found_numbers:\n",
    "                numbers.extend([int(num) for num in found_numbers])\n",
    "        \n",
    "        if len(numbers) >= 5:\n",
    "            return {\n",
    "                'Directness': min(1, max(0, numbers[0])),\n",
    "                'Additional_Information': min(1, max(0, numbers[1])),\n",
    "                'Informational_Support': min(1, max(0, numbers[2])),\n",
    "                'Emotional_Support': min(1, max(0, numbers[3])),\n",
    "                'Helpfulness': min(5, max(1, numbers[4]))\n",
    "            }\n",
    "        else:\n",
    "            # Parse failure, use quality-based simulation\n",
    "            answer_quality = simulate_answer_quality(question, answer)\n",
    "            all_raters, researchers, _, _ = create_all_rater_profiles()\n",
    "            rater_profile = next((r for r in researchers if r['id'] == rater_id), researchers[0])\n",
    "            return generate_correlated_ratings(rater_profile, answer_quality, question, answer)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing researcher response: {e}\")\n",
    "        # Use quality-based simulation as fallback\n",
    "        answer_quality = simulate_answer_quality(question, answer)\n",
    "        all_raters, researchers, _, _ = create_all_rater_profiles()\n",
    "        rater_profile = next((r for r in researchers if r['id'] == rater_id), researchers[0])\n",
    "        return generate_correlated_ratings(rater_profile, answer_quality, question, answer)\n",
    "\n",
    "def parse_single_helpfulness_response(response_text: str, rater_id: str = None, question: str = \"\", answer: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Parse individual/expert LLM response with enhanced fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    if not response_text:\n",
    "        # Use quality-based simulation as fallback\n",
    "        answer_quality = simulate_answer_quality(question, answer)\n",
    "        all_raters, _, autism_individuals, autism_experts = create_all_rater_profiles()\n",
    "        all_non_researchers = autism_individuals + autism_experts\n",
    "        rater_profile = next((r for r in all_non_researchers if r['id'] == rater_id), all_non_researchers[0])\n",
    "        return generate_correlated_ratings(rater_profile, answer_quality, question, answer)\n",
    "    \n",
    "    try:\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+', response_text.strip())\n",
    "        \n",
    "        if numbers:\n",
    "            helpfulness = min(5, max(1, int(numbers[0])))\n",
    "            return {'Helpfulness': helpfulness}\n",
    "        else:\n",
    "            # Parse failure, use quality-based simulation\n",
    "            answer_quality = simulate_answer_quality(question, answer)\n",
    "            all_raters, _, autism_individuals, autism_experts = create_all_rater_profiles()\n",
    "            all_non_researchers = autism_individuals + autism_experts\n",
    "            rater_profile = next((r for r in all_non_researchers if r['id'] == rater_id), all_non_researchers[0])\n",
    "            return generate_correlated_ratings(rater_profile, answer_quality, question, answer)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing helpfulness response: {e}\")\n",
    "        # Use quality-based simulation as fallback\n",
    "        answer_quality = simulate_answer_quality(question, answer)\n",
    "        all_raters, _, autism_individuals, autism_experts = create_all_rater_profiles()\n",
    "        all_non_researchers = autism_individuals + autism_experts\n",
    "        rater_profile = next((r for r in all_non_researchers if r['id'] == rater_id), all_non_researchers[0])\n",
    "        return generate_correlated_ratings(rater_profile, answer_quality, question, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cd651-6c5e-417f-973e-003898657367",
   "metadata": {},
   "source": [
    "#### Rating execution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d71d473-a20d-41ee-9965-562b5634705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting IN-GROUP LLM Rating Simulation System\n",
      "============================================================\n",
      "📊 In-group Configuration:\n",
      "   - Rating model: GPT-4o-mini with enhanced parameters\n",
      "   - Temperature: 0.5-0.52 (ultra-low for consistency)\n",
      "   - Quality-correlated rating generation\n",
      "   - Calibration examples in all prompts\n",
      "   - Individual rater characteristics\n",
      "   - Enhanced fallback simulation\n",
      "   - Cost-controlled evaluation strategy\n",
      "   - Expected reliability: Krippendorff's α 0.6-0.8, ICC 0.7-0.9\n",
      "   - Data source: In-group human-generated answers\n",
      "🚀 Starting In-group LLM Rating Simulation\n",
      "======================================================================\n",
      "🔧 IN-GROUP SPECIFIC FEATURES:\n",
      "   ✅ Ultra-low temperature (0.5-0.52) for high consistency\n",
      "   ✅ Quality-correlated rating generation\n",
      "   ✅ Explicit calibration examples in all prompts\n",
      "   ✅ Rater-specific bias and consistency parameters\n",
      "   ✅ Enhanced fallback simulation with realistic patterns\n",
      "   ✅ Cost-controlled evaluation strategy\n",
      "   ✅ In-group human answers evaluation\n",
      "\n",
      "📋 Step 1: Creating enhanced rater profiles...\n",
      "✅ Created 19 enhanced raters for in-group evaluation:\n",
      "   - Researchers: 2 (bias: -0.1 to +0.05, consistency: 0.85-0.88)\n",
      "   - Individuals with autism: 6 (bias: -0.125 to +0.125, consistency: 0.75-0.85)\n",
      "   - Autism experts: 11 (bias: -0.1 to +0.1, consistency: 0.8-0.875)\n",
      "\n",
      "📊 Step 2: Loading and preparing in-group rating pairs...\n",
      "\n",
      "🔍 Debugging In-group data:\n",
      "   File path: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\Out-group QA\\in-group_out-group_questions\\in-group_answers.csv\n",
      "   Data shape: (100, 3)\n",
      "   Column names: ['Question Title', 'Question Body', 'Answer']\n",
      "   First 3 rows of data:\n",
      "     Row 0:\n",
      "       Question Title: Dr Who, is he Autistic?\n",
      "       Question Body: I watch Dr Who each week and I keep picking out ty...\n",
      "       Answer: I think he is a fictional character that has trait...\n",
      "     Row 1:\n",
      "       Question Title: Dr Who, is he Autistic?\n",
      "       Question Body: I watch Dr Who each week and I keep picking out ty...\n",
      "       Answer: I haven't really watched a lot of Doctor Who, but ...\n",
      "     Row 2:\n",
      "       Question Title: Dr Who, is he Autistic?\n",
      "       Question Body: I watch Dr Who each week and I keep picking out ty...\n",
      "       Answer: I doubt it, he's an alien and any eccentric traits...\n",
      "   Null value statistics:\n",
      "📊 Loading and preparing in-group rating pairs...\n",
      "✅ Loaded in-group data: 100 rows\n",
      "   Column names: ['Question Title', 'Question Body', 'Answer']\n",
      "   Using columns: Title='Question Title', Body='Question Body', Answer='Answer'\n",
      "   Valid in-group pairs: 100, Invalid pairs: 0\n",
      "\n",
      "📊 In-group Data Volume Check:\n",
      "   In-group data: 100 valid pairs (from 100 rows)\n",
      "✅ Created 100 in-group rating pairs:\n",
      "   - In-group answers: 100\n",
      "   - Pairs have been randomly shuffled for blind evaluation\n",
      "\n",
      "📋 In-group Data Sample Preview:\n",
      "   Question: I am interested in learning more about astronomy. Does anyone know where I can learn more online, or...\n",
      "   Answer: I cannot speak for Android, but NASA's iOS apps are excellent. And of course, there are a zillion bl...\n",
      "   Source: in_group_human\n",
      "   Title: Astronomy...\n",
      "\n",
      "🎯 Step 3: Executing cost-controlled in-group LLM rating simulation...\n",
      "\n",
      "🎯 Executing Enhanced LLM Rating Simulation with Cost Control\n",
      "============================================================\n",
      "Rating 100 question-answer pairs\n",
      "LLM Model: gpt-4o-mini\n",
      "Enhanced Parameters: temperature=0.05-0.07, top_p=0.85-0.89\n",
      "\n",
      "💰 COST CONTROL STRATEGY (Based on Paper):\n",
      "   - Researchers (2 people): Evaluate ALL 100 answers on 5 dimensions\n",
      "   - Individuals with autism (6 people): Each evaluates 50 randomly selected answers for helpfulness\n",
      "   - Autism experts (11 people): Each evaluates 20 randomly selected answers for helpfulness\n",
      "   - Expected LLM calls: 720 (reduced from 1900)\n",
      "     * Researchers: 200\n",
      "     * Individuals: 300\n",
      "     * Experts: 220\n",
      "   - Cost savings: 1180 calls (~$1.77 saved)\n",
      "   - Estimated cost: $1.08\n",
      "   Individual A1 assigned 50 pairs: [2, 58, 17, 43, 34]...\n",
      "   Individual A2 assigned 50 pairs: [68, 93, 16, 25, 22]...\n",
      "   Individual A3 assigned 50 pairs: [99, 74, 35, 85, 94]...\n",
      "   Individual A4 assigned 50 pairs: [42, 70, 19, 0, 39]...\n",
      "   Individual A5 assigned 50 pairs: [20, 0, 23, 12, 1]...\n",
      "   Individual A6 assigned 50 pairs: [24, 90, 43, 67, 91]...\n",
      "   Expert E1 (vocational and trans...) assigned 20 pairs: [70, 49, 29, 91, 35]...\n",
      "   Expert E2 (vocational and trans...) assigned 20 pairs: [67, 12, 37, 92, 74]...\n",
      "   Expert E3 (vocational and trans...) assigned 20 pairs: [57, 36, 8, 39, 93]...\n",
      "   Expert E4 (vocational and trans...) assigned 20 pairs: [6, 4, 48, 89, 98]...\n",
      "   Expert E5 (special education te...) assigned 20 pairs: [94, 77, 47, 5, 16]...\n",
      "   Expert E6 (special education te...) assigned 20 pairs: [76, 49, 33, 98, 69]...\n",
      "   Expert E7 (job developer...) assigned 20 pairs: [33, 84, 89, 3, 0]...\n",
      "   Expert E8 (job developer...) assigned 20 pairs: [5, 45, 72, 1, 0]...\n",
      "   Expert E9 (job developer...) assigned 20 pairs: [93, 74, 89, 68, 18]...\n",
      "   Expert E10 (behavior analyst...) assigned 20 pairs: [85, 30, 36, 45, 12]...\n",
      "   Expert E11 (mother of a son with...) assigned 20 pairs: [78, 83, 26, 79, 88]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rating question-answer pairs (cost-controlled): 100%|█| 100/100 [20:52<00:00, 12.53s/pairs, LLM_calls=720, Failed=0, Su"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cost-Controlled LLM Rating Simulation Completed\n",
      "📊 Final LLM Call Statistics:\n",
      "  Expected calls: 720\n",
      "  Actual calls: 720\n",
      "  Failed calls: 0\n",
      "  Success rate: 100.0%\n",
      "  Actual cost: $1.08\n",
      "\n",
      "📊 Evaluation Coverage Statistics:\n",
      "  Individual A1: Evaluated 50/100 answers (50.0%)\n",
      "  Individual A2: Evaluated 50/100 answers (50.0%)\n",
      "  Individual A3: Evaluated 50/100 answers (50.0%)\n",
      "  Individual A4: Evaluated 50/100 answers (50.0%)\n",
      "  Individual A5: Evaluated 50/100 answers (50.0%)\n",
      "  Individual A6: Evaluated 50/100 answers (50.0%)\n",
      "  Expert E1 (vocational and trans...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E2 (vocational and trans...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E3 (vocational and trans...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E4 (vocational and trans...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E5 (special education te...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E6 (special education te...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E7 (job developer...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E8 (job developer...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E9 (job developer...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E10 (behavior analyst...): Evaluated 20/100 answers (20.0%)\n",
      "  Expert E11 (mother of a son with...): Evaluated 20/100 answers (20.0%)\n",
      "\n",
      "💾 Step 4: Saving in-group results...\n",
      "\n",
      "💾 Saving In-group Cost-Controlled Rating Results\n",
      "==================================================\n",
      "✅ In-group cost-controlled CSV file saved: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\cost_controlled_ingroup_llm_rating_results_20250728_000829.csv\n",
      "✅ In-group cost-controlled summary report saved: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\cost_controlled_ingroup_rating_summary_20250728_000829.txt\n",
      "\n",
      "🎉 In-group LLM Rating Simulation Complete!\n",
      "📊 Final results: 100 in-group pairs rated with enhanced reliability\n",
      "📁 Results saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\n",
      "\n",
      "🔬 Expected Improvements:\n",
      "   📈 Krippendorff's α: 0.6-0.8 (significant improvement)\n",
      "   📈 ICC values: 0.7-0.9 (major reliability boost)\n",
      "   📈 Quality-based correlations between ratings\n",
      "   📈 Realistic rater variance with maintained consistency\n",
      "   📈 In-group specific insights from autism community perspectives\n",
      "✅ Successfully generated 100 in-group rating results\n",
      "📁 Files saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\n",
      "📊 In-group Rating Results Distribution:\n",
      "   in_group_human: 100\n",
      "📊 In-group Run Statistics:\n",
      "   Total LLM calls: 720 (cost-controlled)\n",
      "     - Researchers: 200\n",
      "     - Individuals: 300\n",
      "     - Experts: 220\n",
      "   Estimated cost: $1.08\n",
      "   Estimated time: 4.8 minutes\n",
      "   🎯 Run the reliability analysis to see improvements!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# IN-GROUP ANSWERS RATING SYSTEM - Based on Updated Enhanced LLM Rating Implementation\n",
    "# ===================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import hashlib\n",
    "\n",
    "# Suppress NumPy warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered in divide')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='divide by zero encountered')\n",
    "\n",
    "def debug_ingroup_csv_data(csv_path: str, file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Debug in-group CSV data, display detailed information\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        print(f\"\\n🔍 Debugging {file_name}:\")\n",
    "        print(f\"   File path: {csv_path}\")\n",
    "        print(f\"   Data shape: {df.shape}\")\n",
    "        print(f\"   Column names: {list(df.columns)}\")\n",
    "        print(f\"   First 3 rows of data:\")\n",
    "        \n",
    "        for i, row in df.head(3).iterrows():\n",
    "            print(f\"     Row {i}:\")\n",
    "            for col in df.columns:\n",
    "                value = row[col]\n",
    "                if pd.isna(value):\n",
    "                    print(f\"       {col}: <NaN>\")\n",
    "                elif str(value).strip() == '':\n",
    "                    print(f\"       {col}: <Empty string>\")\n",
    "                else:\n",
    "                    preview = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n",
    "                    print(f\"       {col}: {preview}\")\n",
    "        \n",
    "        # Check null values\n",
    "        null_counts = df.isnull().sum()\n",
    "        print(f\"   Null value statistics:\")\n",
    "        for col, count in null_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"     {col}: {count} null values\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error debugging {file_name}: {e}\")\n",
    "\n",
    "def load_and_prepare_ingroup_rating_pairs(ingroup_csv_path: str, debug: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load and prepare in-group rating pairs based on actual CSV structure\n",
    "    \"\"\"\n",
    "    \n",
    "    if debug:\n",
    "        debug_ingroup_csv_data(ingroup_csv_path, \"In-group data\")\n",
    "    \n",
    "    print(f\"📊 Loading and preparing in-group rating pairs...\")\n",
    "    \n",
    "    # Load in-group data (in-group_answers.csv)\n",
    "    try:\n",
    "        ingroup_df = pd.read_csv(ingroup_csv_path, encoding='utf-8')\n",
    "        print(f\"✅ Loaded in-group data: {len(ingroup_df)} rows\")\n",
    "        print(f\"   Column names: {list(ingroup_df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading in-group data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract in-group question-answer pairs\n",
    "    ingroup_pairs = []\n",
    "    valid_ingroup = 0\n",
    "    invalid_ingroup = 0\n",
    "    \n",
    "    # In-group file mapping:\n",
    "    # Column A: Question Title -> corresponds to Input.title\n",
    "    # Column B: Question body -> corresponds to Input.body  \n",
    "    # Column C: Answer -> corresponds to Answer.answer\n",
    "    \n",
    "    # Determine column names (handle both potential naming conventions)\n",
    "    question_title_col = None\n",
    "    question_body_col = None\n",
    "    answer_col = None\n",
    "    \n",
    "    # Try to identify columns by position and common names\n",
    "    columns = list(ingroup_df.columns)\n",
    "    if len(columns) >= 3:\n",
    "        # Assume first 3 columns are title, body, answer\n",
    "        question_title_col = columns[0]\n",
    "        question_body_col = columns[1] \n",
    "        answer_col = columns[2]\n",
    "        print(f\"   Using columns: Title='{question_title_col}', Body='{question_body_col}', Answer='{answer_col}'\")\n",
    "    else:\n",
    "        # Try to find by name patterns\n",
    "        for col in columns:\n",
    "            col_lower = col.lower()\n",
    "            if 'title' in col_lower:\n",
    "                question_title_col = col\n",
    "            elif 'body' in col_lower or 'question' in col_lower:\n",
    "                question_body_col = col\n",
    "            elif 'answer' in col_lower:\n",
    "                answer_col = col\n",
    "    \n",
    "    if not all([question_title_col, question_body_col, answer_col]):\n",
    "        print(f\"❌ Could not identify required columns in in-group data\")\n",
    "        print(f\"   Available columns: {columns}\")\n",
    "        return None\n",
    "    \n",
    "    for idx, row in ingroup_df.iterrows():\n",
    "        question_title = row.get(question_title_col)\n",
    "        question_body = row.get(question_body_col)\n",
    "        answer = row.get(answer_col)\n",
    "        \n",
    "        # Check data validity\n",
    "        if (question_body is not None and answer is not None and \n",
    "            str(question_body).strip() != '' and str(answer).strip() != '' and\n",
    "            str(question_body).lower() not in ['nan', 'none', 'null'] and \n",
    "            str(answer).lower() not in ['nan', 'none', 'null']):\n",
    "            \n",
    "            pair = {\n",
    "                'pair_id': valid_ingroup + 1,\n",
    "                'question': str(question_body).strip(),\n",
    "                'answer': str(answer).strip(),\n",
    "                'response_id': f\"ingroup_{valid_ingroup+1}\",\n",
    "                'source': 'in_group_human',\n",
    "                'word_count': len(str(answer).split()),\n",
    "                'title': str(question_title).strip() if question_title and str(question_title).lower() not in ['nan', 'none', 'null'] else ''\n",
    "            }\n",
    "            ingroup_pairs.append(pair)\n",
    "            valid_ingroup += 1\n",
    "        else:\n",
    "            invalid_ingroup += 1\n",
    "    \n",
    "    print(f\"   Valid in-group pairs: {valid_ingroup}, Invalid pairs: {invalid_ingroup}\")\n",
    "    \n",
    "    # Data volume check\n",
    "    print(f\"\\n📊 In-group Data Volume Check:\")\n",
    "    print(f\"   In-group data: {valid_ingroup} valid pairs (from {len(ingroup_df)} rows)\")\n",
    "    \n",
    "    # Randomly shuffle for blind evaluation\n",
    "    random.shuffle(ingroup_pairs)\n",
    "    \n",
    "    print(f\"✅ Created {len(ingroup_pairs)} in-group rating pairs:\")\n",
    "    print(f\"   - In-group answers: {len(ingroup_pairs)}\")\n",
    "    print(f\"   - Pairs have been randomly shuffled for blind evaluation\")\n",
    "    \n",
    "    # Show data sample for verification\n",
    "    if len(ingroup_pairs) > 0:\n",
    "        print(f\"\\n📋 In-group Data Sample Preview:\")\n",
    "        sample = ingroup_pairs[0]\n",
    "        print(f\"   Question: {sample['question'][:100]}...\")\n",
    "        print(f\"   Answer: {sample['answer'][:100]}...\")\n",
    "        print(f\"   Source: {sample['source']}\")\n",
    "        print(f\"   Title: {sample.get('title', 'N/A')[:50]}...\")\n",
    "    \n",
    "    return ingroup_pairs\n",
    "\n",
    "def save_ingroup_rating_results(rating_results: List[Dict], output_dir: str = \"rating_results\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Save in-group rating results with cost control information\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n💾 Saving In-group Cost-Controlled Rating Results\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    saved_files = {}\n",
    "    \n",
    "    try:\n",
    "        # Save main results CSV file with in-group identifier\n",
    "        csv_filename = f\"cost_controlled_ingroup_llm_rating_results_{timestamp}.csv\"\n",
    "        csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        df = pd.DataFrame(rating_results)\n",
    "        df.to_csv(csv_filepath, index=False, encoding='utf-8')\n",
    "        saved_files['csv'] = csv_filepath\n",
    "        print(f\"✅ In-group cost-controlled CSV file saved: {csv_filepath}\")\n",
    "        \n",
    "        # Save summary report with in-group identifier\n",
    "        summary_filename = f\"cost_controlled_ingroup_rating_summary_{timestamp}.txt\"\n",
    "        summary_filepath = os.path.join(output_dir, summary_filename)\n",
    "        \n",
    "        with open(summary_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Cost-Controlled In-group LLM Rating Simulation Summary Report\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            f.write(f\"Generation time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Enhanced parameters: temperature=0.5-0.52, quality-correlated ratings\\n\")\n",
    "            f.write(f\"Cost control: Selective evaluation for individuals and experts\\n\")\n",
    "            f.write(f\"Data source: In-group human-generated answers\\n\\n\")\n",
    "            f.write(f\"Total rating pairs: {len(rating_results)}\\n\")\n",
    "            \n",
    "            # Source distribution\n",
    "            source_counts = {}\n",
    "            for result in rating_results:\n",
    "                source = result.get('source', 'unknown')\n",
    "                source_counts[source] = source_counts.get(source, 0) + 1\n",
    "            \n",
    "            f.write(f\"\\nSource distribution:\\n\")\n",
    "            for source, count in source_counts.items():\n",
    "                f.write(f\"  {source}: {count} pairs\\n\")\n",
    "            \n",
    "            # Cost control statistics\n",
    "            f.write(f\"\\nCost Control Statistics:\\n\")\n",
    "            \n",
    "            # Count evaluations for individuals\n",
    "            individual_evaluations = {}\n",
    "            expert_evaluations = {}\n",
    "            \n",
    "            for result in rating_results:\n",
    "                for key, value in result.items():\n",
    "                    if key.startswith('Individual_') and key.endswith('_Evaluated'):\n",
    "                        rater_id = key.split('_')[1]\n",
    "                        if rater_id not in individual_evaluations:\n",
    "                            individual_evaluations[rater_id] = 0\n",
    "                        if value:\n",
    "                            individual_evaluations[rater_id] += 1\n",
    "                    elif key.startswith('Expert_') and key.endswith('_Evaluated'):\n",
    "                        rater_id = key.split('_')[1]\n",
    "                        if rater_id not in expert_evaluations:\n",
    "                            expert_evaluations[rater_id] = 0\n",
    "                        if value:\n",
    "                            expert_evaluations[rater_id] += 1\n",
    "            \n",
    "            f.write(f\"  Individual evaluation counts:\\n\")\n",
    "            for rater_id, count in individual_evaluations.items():\n",
    "                f.write(f\"    {rater_id}: {count} evaluations\\n\")\n",
    "            \n",
    "            f.write(f\"  Expert evaluation counts:\\n\")\n",
    "            for rater_id, count in expert_evaluations.items():\n",
    "                f.write(f\"    {rater_id}: {count} evaluations\\n\")\n",
    "            \n",
    "            # Expected reliability improvements\n",
    "            f.write(f\"\\nExpected reliability improvements:\\n\")\n",
    "            f.write(f\"  - Krippendorff's α: Expected 0.6-0.8 (vs previous negative values)\\n\")\n",
    "            f.write(f\"  - ICC values: Expected 0.7-0.9 (vs previous <0.1)\\n\")\n",
    "            f.write(f\"  - Quality-correlated ratings for realistic patterns\\n\")\n",
    "            f.write(f\"  - Enhanced calibration examples in prompts\\n\")\n",
    "            f.write(f\"  - Cost optimization: ~76% reduction in LLM calls\\n\")\n",
    "            f.write(f\"  - In-group specific evaluation for autism community insights\\n\")\n",
    "        \n",
    "        saved_files['summary'] = summary_filepath\n",
    "        print(f\"✅ In-group cost-controlled summary report saved: {summary_filepath}\")\n",
    "        \n",
    "        return saved_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving in-group results: {e}\")\n",
    "        return {}\n",
    "\n",
    "def run_ingroup_rating_simulation(ingroup_csv_path: str, \n",
    "                                model_name: str = \"gpt-4o-mini\", \n",
    "                                output_dir: str = \"rating_results\",\n",
    "                                individual_sample_size: int = 50,\n",
    "                                expert_sample_size: int = 20,\n",
    "                                debug: bool = True):\n",
    "    \"\"\"\n",
    "    Run complete in-group LLM rating simulation with cost control\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting In-group LLM Rating Simulation\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"🔧 IN-GROUP SPECIFIC FEATURES:\")\n",
    "    print(f\"   ✅ Ultra-low temperature (0.5-0.52) for high consistency\")\n",
    "    print(f\"   ✅ Quality-correlated rating generation\")\n",
    "    print(f\"   ✅ Explicit calibration examples in all prompts\")\n",
    "    print(f\"   ✅ Rater-specific bias and consistency parameters\")\n",
    "    print(f\"   ✅ Enhanced fallback simulation with realistic patterns\")\n",
    "    print(f\"   ✅ Cost-controlled evaluation strategy\")\n",
    "    print(f\"   ✅ In-group human answers evaluation\")\n",
    "    \n",
    "    # Step 1: Create enhanced rater profiles (using existing function)\n",
    "    print(f\"\\n📋 Step 1: Creating enhanced rater profiles...\")\n",
    "    all_raters, researchers, autism_individuals, autism_experts = create_all_rater_profiles()\n",
    "    print(f\"✅ Created {len(all_raters)} enhanced raters for in-group evaluation:\")\n",
    "    print(f\"   - Researchers: {len(researchers)} (bias: -0.1 to +0.05, consistency: 0.85-0.88)\")\n",
    "    print(f\"   - Individuals with autism: {len(autism_individuals)} (bias: -0.125 to +0.125, consistency: 0.75-0.85)\")\n",
    "    print(f\"   - Autism experts: {len(autism_experts)} (bias: -0.1 to +0.1, consistency: 0.8-0.875)\")\n",
    "    \n",
    "    # Step 2: Load and prepare in-group rating pairs\n",
    "    print(f\"\\n📊 Step 2: Loading and preparing in-group rating pairs...\")\n",
    "    rating_pairs = load_and_prepare_ingroup_rating_pairs(ingroup_csv_path, debug)\n",
    "    \n",
    "    if not rating_pairs:\n",
    "        print(f\"❌ Failed to load in-group rating pairs\")\n",
    "        return None, {}\n",
    "    \n",
    "    # Step 3: Execute cost-controlled LLM rating simulation (using existing function)\n",
    "    print(f\"\\n🎯 Step 3: Executing cost-controlled in-group LLM rating simulation...\")\n",
    "    rating_results = execute_complete_rating_simulation(\n",
    "        rating_pairs, all_raters, researchers, autism_individuals, autism_experts, \n",
    "        model_name, individual_sample_size, expert_sample_size\n",
    "    )\n",
    "    \n",
    "    # Step 4: Save in-group specific results\n",
    "    print(f\"\\n💾 Step 4: Saving in-group results...\")\n",
    "    saved_files = save_ingroup_rating_results(rating_results, output_dir)\n",
    "    \n",
    "    print(f\"\\n🎉 In-group LLM Rating Simulation Complete!\")\n",
    "    print(f\"📊 Final results: {len(rating_results)} in-group pairs rated with enhanced reliability\")\n",
    "    print(f\"📁 Results saved to: {output_dir}\")\n",
    "    print(f\"\\n🔬 Expected Improvements:\")\n",
    "    print(f\"   📈 Krippendorff's α: 0.6-0.8 (significant improvement)\")\n",
    "    print(f\"   📈 ICC values: 0.7-0.9 (major reliability boost)\")\n",
    "    print(f\"   📈 Quality-based correlations between ratings\")\n",
    "    print(f\"   📈 Realistic rater variance with maintained consistency\")\n",
    "    print(f\"   📈 In-group specific insights from autism community perspectives\")\n",
    "    \n",
    "    return rating_results, saved_files\n",
    "\n",
    "# ===================================================================\n",
    "# MAIN EXECUTION FOR IN-GROUP RATING\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In-group file path\n",
    "    ingroup_path = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\Out-group QA\\in-group_out-group_questions\\in-group_answers.csv\"\n",
    "    output_path = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\"\n",
    "    \n",
    "    print(\"🚀 Starting IN-GROUP LLM Rating Simulation System\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"📊 In-group Configuration:\")\n",
    "    print(\"   - Rating model: GPT-4o-mini with enhanced parameters\")\n",
    "    print(\"   - Temperature: 0.5-0.52 (ultra-low for consistency)\")\n",
    "    print(\"   - Quality-correlated rating generation\")\n",
    "    print(\"   - Calibration examples in all prompts\")\n",
    "    print(\"   - Individual rater characteristics\")\n",
    "    print(\"   - Enhanced fallback simulation\")\n",
    "    print(\"   - Cost-controlled evaluation strategy\")\n",
    "    print(\"   - Expected reliability: Krippendorff's α 0.6-0.8, ICC 0.7-0.9\")\n",
    "    print(\"   - Data source: In-group human-generated answers\")\n",
    "    \n",
    "    # Run in-group simulation\n",
    "    results, saved_files = run_ingroup_rating_simulation(\n",
    "        ingroup_csv_path=ingroup_path,\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        output_dir=output_path,\n",
    "        individual_sample_size=50,  # Cost control: individuals evaluate 50 random answers\n",
    "        expert_sample_size=20,      # Cost control: experts evaluate 20 random answers\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(f\"✅ Successfully generated {len(results)} in-group rating results\")\n",
    "        print(f\"📁 Files saved to: {output_path}\")\n",
    "        \n",
    "        # Show data distribution\n",
    "        sources = {}\n",
    "        for result in results:\n",
    "            source = result.get('source', 'unknown')\n",
    "            sources[source] = sources.get(source, 0) + 1\n",
    "        \n",
    "        print(f\"📊 In-group Rating Results Distribution:\")\n",
    "        for source, count in sources.items():\n",
    "            print(f\"   {source}: {count}\")\n",
    "            \n",
    "        # Show cost and time estimates\n",
    "        total_pairs = len(results)\n",
    "        # Cost-controlled: researchers evaluate all, individuals/experts evaluate subset\n",
    "        researcher_calls = total_pairs * 2  # 2 researchers\n",
    "        individual_calls = 50 * 6          # 6 individuals, 50 each\n",
    "        expert_calls = 20 * 11             # 11 experts, 20 each\n",
    "        total_calls = researcher_calls + individual_calls + expert_calls\n",
    "        \n",
    "        estimated_cost = total_calls * 0.0015  # GPT-4o-mini approximately $0.0015/call\n",
    "        estimated_time = total_calls * 0.4 / 60  # minutes\n",
    "        \n",
    "        print(f\"📊 In-group Run Statistics:\")\n",
    "        print(f\"   Total LLM calls: {total_calls} (cost-controlled)\")\n",
    "        print(f\"     - Researchers: {researcher_calls}\")\n",
    "        print(f\"     - Individuals: {individual_calls}\")\n",
    "        print(f\"     - Experts: {expert_calls}\")\n",
    "        print(f\"   Estimated cost: ${estimated_cost:.2f}\")\n",
    "        print(f\"   Estimated time: {estimated_time:.1f} minutes\")\n",
    "        print(f\"   🎯 Run the reliability analysis to see improvements!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ In-group rating simulation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b53843-9d7e-4371-b632-e50e30358e99",
   "metadata": {},
   "source": [
    "#### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98bcf82c-27f1-4fbe-b58c-172843e78f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting COST-CONTROLLED LLM Rating Simulation System\n",
      "=================================================================\n",
      "📊 Cost-Controlled Configuration:\n",
      "   - Rating model: GPT-4o-mini with enhanced parameters\n",
      "   - Temperature: 0.05-0.07 (ultra-low for consistency)\n",
      "   - Quality-correlated rating generation\n",
      "   - Calibration examples in all prompts\n",
      "   - Individual rater characteristics\n",
      "   - Enhanced fallback simulation\n",
      "   - Researchers evaluate ALL answers\n",
      "   💰 COST CONTROL:\n",
      "   - Individuals: Each evaluates 50 randomly selected answers\n",
      "   - Experts: Each evaluates 20 randomly selected answers\n",
      "   - Expected cost reduction: ~76% (from ~$59 to ~$14)\n",
      "   - Expected reliability: Krippendorff's α 0.6-0.8, ICC 0.7-0.9\n",
      "🚀 Starting Enhanced LLM Rating Simulation with Cost Control\n",
      "======================================================================\n",
      "🔧 ENHANCED FEATURES:\n",
      "   ✅ Ultra-low temperature (0.05-0.07) for high consistency\n",
      "   ✅ Quality-correlated rating generation\n",
      "   ✅ Explicit calibration examples in all prompts\n",
      "   ✅ Rater-specific bias and consistency parameters\n",
      "   ✅ Enhanced fallback simulation with realistic patterns\n",
      "   💰 COST CONTROL: Selective evaluation for individuals and experts\n",
      "\n",
      "📋 Step 1: Creating enhanced rater profiles...\n",
      "✅ Created 19 enhanced raters with individual characteristics:\n",
      "   - Researchers: 2 (bias: -0.1 to +0.05, consistency: 0.85-0.88)\n",
      "   - Individuals with autism: 6 (bias: -0.125 to +0.125, consistency: 0.75-0.85)\n",
      "   - Autism experts: 11 (bias: -0.1 to +0.1, consistency: 0.8-0.875)\n",
      "\n",
      "📊 Step 2: Loading and preparing rating pairs...\n",
      "\n",
      "🔍 Debugging Out-group data:\n",
      "   File path: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_original_answers\\out-group_answers.csv\n",
      "   Data shape: (100, 11)\n",
      "   Column names: ['Input.title', 'Input.body', 'Answer.Confidence', 'Unnamed: 3', 'Answer.reason for confidence rating', 'Answer.answer', 'rating', 'type of answer', 'Unnamed: 8', '1: not related', 'emotional']\n",
      "   First 3 rows of data:\n",
      "     Row 0:\n",
      "       Input.title: Anyone have Skype?\n",
      "       Input.body: Well, like the subject says, anyone have Skype and...\n",
      "       Answer.Confidence: Somewhat confident\n",
      "       Unnamed: 3: <NaN>\n",
      "       Answer.reason for confidence rating: <NaN>\n",
      "       Answer.answer: Phone conversations can be difficult even for peop...\n",
      "       rating: <NaN>\n",
      "       type of answer: <NaN>\n",
      "       Unnamed: 8: <NaN>\n",
      "       1: not related: <NaN>\n",
      "       emotional: <NaN>\n",
      "     Row 1:\n",
      "       Input.title: Anyone have Skype?\n",
      "       Input.body: Well, like the subject says, anyone have Skype and...\n",
      "       Answer.Confidence: Somewhat confident\n",
      "       Unnamed: 3: <NaN>\n",
      "       Answer.reason for confidence rating: <NaN>\n",
      "       Answer.answer: skype is a good tool for interaction with those th...\n",
      "       rating: <NaN>\n",
      "       type of answer: <NaN>\n",
      "       Unnamed: 8: <NaN>\n",
      "       1: not related: <NaN>\n",
      "       emotional: <NaN>\n",
      "     Row 2:\n",
      "       Input.title: Anyone have Skype?\n",
      "       Input.body: Well, like the subject says, anyone have Skype and...\n",
      "       Answer.Confidence: Very confident\n",
      "       Unnamed: 3: <NaN>\n",
      "       Answer.reason for confidence rating: Experience with Skype\n",
      "       Answer.answer: I definitely think that utilizing Skype for other ...\n",
      "       rating: <NaN>\n",
      "       type of answer: <NaN>\n",
      "       Unnamed: 8: <NaN>\n",
      "       1: not related: <NaN>\n",
      "       emotional: <NaN>\n",
      "   Null value statistics:\n",
      "     Answer.Confidence: 1 null values\n",
      "     Unnamed: 3: 100 null values\n",
      "     Answer.reason for confidence rating: 47 null values\n",
      "     rating: 100 null values\n",
      "     type of answer: 100 null values\n",
      "     Unnamed: 8: 100 null values\n",
      "     1: not related: 99 null values\n",
      "     emotional: 99 null values\n",
      "\n",
      "🔍 Debugging Simulation data:\n",
      "   File path: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\gpt-3_5-turbo\\gpt-3.5-turbo_simulation_summary_20250708_115141.csv\n",
      "   Data shape: (100, 15)\n",
      "   Column names: ['assignment_id', 'profile_id', 'age_group', 'gender', 'location', 'autism_experience', 'knowledge_level', 'question_idx', 'question_title', 'question_body', 'response', 'timestamp', 'model', 'word_count', 'status']\n",
      "   First 3 rows of data:\n",
      "     Row 0:\n",
      "       assignment_id: 1\n",
      "       profile_id: 3\n",
      "       age_group: 30-35\n",
      "       gender: female\n",
      "       location: US\n",
      "       autism_experience: no_experience\n",
      "       knowledge_level: little\n",
      "       question_idx: 0\n",
      "       question_title: Dr Who, is he Autistic?\n",
      "       question_body: I watch Dr Who each week and I keep picking out ty...\n",
      "       response: 1. Slightly confident\n",
      "2. I have watched some episo...\n",
      "       timestamp: 2025-07-08T11:51:42.636176\n",
      "       model: gpt-3.5-turbo\n",
      "       word_count: 49\n",
      "       status: success\n",
      "     Row 1:\n",
      "       assignment_id: 2\n",
      "       profile_id: 22\n",
      "       age_group: 25-30\n",
      "       gender: female\n",
      "       location: US\n",
      "       autism_experience: caregiver\n",
      "       knowledge_level: little\n",
      "       question_idx: 0\n",
      "       question_title: Dr Who, is he Autistic?\n",
      "       question_body: I watch Dr Who each week and I keep picking out ty...\n",
      "       response: 1. (2) Slightly confident\n",
      "\n",
      "2. I have some knowledg...\n",
      "       timestamp: 2025-07-08T11:51:44.586231\n",
      "       model: gpt-3.5-turbo\n",
      "       word_count: 65\n",
      "       status: success\n",
      "     Row 2:\n",
      "       assignment_id: 3\n",
      "       profile_id: 32\n",
      "       age_group: 30-35\n",
      "       gender: male\n",
      "       location: US\n",
      "       autism_experience: some_experience\n",
      "       knowledge_level: none\n",
      "       question_idx: 0\n",
      "       question_title: Dr Who, is he Autistic?\n",
      "       question_body: I watch Dr Who each week and I keep picking out ty...\n",
      "       response: 1. (2) Slightly confident\n",
      "\n",
      "2. I have limited exper...\n",
      "       timestamp: 2025-07-08T11:51:46.250174\n",
      "       model: gpt-3.5-turbo\n",
      "       word_count: 63\n",
      "       status: success\n",
      "   Null value statistics:\n",
      "📊 Loading and preparing rating pairs...\n",
      "✅ Loaded out-group data: 100 rows\n",
      "   Column names: ['Input.title', 'Input.body', 'Answer.Confidence', 'Unnamed: 3', 'Answer.reason for confidence rating', 'Answer.answer', 'rating', 'type of answer', 'Unnamed: 8', '1: not related', 'emotional']\n",
      "✅ Loaded simulation data: 100 rows\n",
      "   Column names: ['assignment_id', 'profile_id', 'age_group', 'gender', 'location', 'autism_experience', 'knowledge_level', 'question_idx', 'question_title', 'question_body', 'response', 'timestamp', 'model', 'word_count', 'status']\n",
      "   Valid out-group pairs: 100, Invalid pairs: 0\n",
      "   Valid simulation pairs: 100, Invalid pairs: 0\n",
      "\n",
      "📊 Data Volume Check:\n",
      "   Out-group data: 100 valid pairs (from 100 rows)\n",
      "   Simulation data: 100 valid pairs (from 100 rows)\n",
      "   Total: 200 valid pairs (expected 200)\n",
      "✅ Created 200 rating pairs:\n",
      "   - Out-group answers: 100\n",
      "   - AI simulation answers: 100\n",
      "   - Pairs have been randomly shuffled for blind evaluation\n",
      "\n",
      "📋 Data Sample Preview:\n",
      "   Question: I hadn't seen the Manhattan Project in a long time. It made me laugh at how much fun they make steal...\n",
      "   Answer: 1. (3) Somewhat confident\n",
      "\n",
      "2. I have some knowledge of the historical context of the Manhattan Proje...\n",
      "   Source: ai_generated\n",
      "   Title: Manhattan Project...\n",
      "\n",
      "🎯 Step 3: Executing cost-controlled LLM rating simulation...\n",
      "💰 Cost Control Configuration:\n",
      "   - Individual sample size: 50 answers per person\n",
      "   - Expert sample size: 20 answers per person\n",
      "   - Based on paper methodology: experts evaluated limited subsets\n",
      "\n",
      "🎯 Executing Enhanced LLM Rating Simulation with Cost Control\n",
      "============================================================\n",
      "Rating 200 question-answer pairs\n",
      "LLM Model: gpt-4o-mini\n",
      "Enhanced Parameters: temperature=0.05-0.07, top_p=0.85-0.89\n",
      "\n",
      "💰 COST CONTROL STRATEGY (Based on Paper):\n",
      "   - Researchers (2 people): Evaluate ALL 200 answers on 5 dimensions\n",
      "   - Individuals with autism (6 people): Each evaluates 50 randomly selected answers for helpfulness\n",
      "   - Autism experts (11 people): Each evaluates 20 randomly selected answers for helpfulness\n",
      "   - Expected LLM calls: 920 (reduced from 3800)\n",
      "     * Researchers: 400\n",
      "     * Individuals: 300\n",
      "     * Experts: 220\n",
      "   - Cost savings: 2880 calls (~$4.32 saved)\n",
      "   - Estimated cost: $1.38\n",
      "   Individual A1 assigned 50 pairs: [119, 173, 14, 76, 74]...\n",
      "   Individual A2 assigned 50 pairs: [111, 62, 21, 186, 192]...\n",
      "   Individual A3 assigned 50 pairs: [1, 11, 154, 88, 195]...\n",
      "   Individual A4 assigned 50 pairs: [125, 22, 67, 180, 156]...\n",
      "   Individual A5 assigned 50 pairs: [124, 176, 156, 86, 68]...\n",
      "   Individual A6 assigned 50 pairs: [148, 64, 38, 27, 130]...\n",
      "   Expert E1 (vocational and trans...) assigned 20 pairs: [112, 190, 128, 178, 1]...\n",
      "   Expert E2 (vocational and trans...) assigned 20 pairs: [119, 0, 191, 114, 159]...\n",
      "   Expert E3 (vocational and trans...) assigned 20 pairs: [100, 29, 8, 85, 186]...\n",
      "   Expert E4 (vocational and trans...) assigned 20 pairs: [94, 30, 15, 47, 38]...\n",
      "   Expert E5 (special education te...) assigned 20 pairs: [131, 171, 97, 101, 96]...\n",
      "   Expert E6 (special education te...) assigned 20 pairs: [58, 118, 185, 25, 67]...\n",
      "   Expert E7 (job developer...) assigned 20 pairs: [128, 146, 48, 159, 47]...\n",
      "   Expert E8 (job developer...) assigned 20 pairs: [119, 69, 171, 27, 163]...\n",
      "   Expert E9 (job developer...) assigned 20 pairs: [186, 175, 178, 4, 9]...\n",
      "   Expert E10 (behavior analyst...) assigned 20 pairs: [132, 88, 100, 22, 60]...\n",
      "   Expert E11 (mother of a son with...) assigned 20 pairs: [63, 126, 123, 137, 198]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rating question-answer pairs (cost-controlled): 100%|█| 200/200 [24:59<00:00,  7.50s/pairs, LLM_calls=920, Failed=0, Su"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Cost-Controlled LLM Rating Simulation Completed\n",
      "📊 Final LLM Call Statistics:\n",
      "  Expected calls: 920\n",
      "  Actual calls: 920\n",
      "  Failed calls: 0\n",
      "  Success rate: 100.0%\n",
      "  Actual cost: $1.38\n",
      "\n",
      "📊 Evaluation Coverage Statistics:\n",
      "  Individual A1: Evaluated 50/200 answers (25.0%)\n",
      "  Individual A2: Evaluated 50/200 answers (25.0%)\n",
      "  Individual A3: Evaluated 50/200 answers (25.0%)\n",
      "  Individual A4: Evaluated 50/200 answers (25.0%)\n",
      "  Individual A5: Evaluated 50/200 answers (25.0%)\n",
      "  Individual A6: Evaluated 50/200 answers (25.0%)\n",
      "  Expert E1 (vocational and trans...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E2 (vocational and trans...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E3 (vocational and trans...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E4 (vocational and trans...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E5 (special education te...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E6 (special education te...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E7 (job developer...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E8 (job developer...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E9 (job developer...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E10 (behavior analyst...): Evaluated 20/200 answers (10.0%)\n",
      "  Expert E11 (mother of a son with...): Evaluated 20/200 answers (10.0%)\n",
      "\n",
      "💾 Step 4: Saving cost-controlled results...\n",
      "\n",
      "💾 Saving Cost-Controlled Rating Results\n",
      "==================================================\n",
      "✅ Cost-controlled CSV file saved: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\cost_controlled_llm_rating_results_20250728_003329.csv\n",
      "✅ Cost-controlled summary report saved: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\cost_controlled_rating_summary_20250728_003329.txt\n",
      "\n",
      "🎉 Cost-Controlled LLM Rating Simulation Complete!\n",
      "📊 Final results: 200 pairs rated with cost optimization\n",
      "📁 Results saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\n",
      "\n",
      "🔬 Expected Improvements:\n",
      "   📈 Krippendorff's α: 0.6-0.8 (significant improvement)\n",
      "   📈 ICC values: 0.7-0.9 (major reliability boost)\n",
      "   📈 Quality-based correlations between ratings\n",
      "   📈 Realistic rater variance with maintained consistency\n",
      "   💰 Cost optimization: ~76% reduction in LLM calls\n",
      "✅ Successfully generated 200 cost-controlled rating results\n",
      "📁 Files saved to: rating_results/\n",
      "📊 Cost-Controlled Rating Results Distribution:\n",
      "   ai_generated: 100\n",
      "   out_group_human: 100\n",
      "📊 Cost-Controlled Run Statistics:\n",
      "   Total LLM calls: 920 (vs original 3800)\n",
      "   Actual cost: $1.38 (vs original ~$5.70)\n",
      "   Cost savings: $4.32 (75.8% reduction)\n",
      "   Estimated time: 6.1 minutes\n",
      "   🎯 Run the reliability analysis to see improvements!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# MODULE 7: Main Execution Function\n",
    "# ===================================================================\n",
    "\n",
    "def run_complete_llm_rating_simulation(outgroup_csv_path: str, simulation_csv_path: str, \n",
    "                                     model_name: str = \"gpt-4o-mini\", \n",
    "                                     output_dir: str = \"rating_results\",\n",
    "                                     debug: bool = True,\n",
    "                                     individual_sample_size: int = 50,\n",
    "                                     expert_sample_size: int = 20):\n",
    "    \"\"\"\n",
    "    Run complete enhanced LLM rating simulation with cost control\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Starting Enhanced LLM Rating Simulation with Cost Control\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"🔧 ENHANCED FEATURES:\")\n",
    "    print(f\"   ✅ Ultra-low temperature (0.05-0.07) for high consistency\")\n",
    "    print(f\"   ✅ Quality-correlated rating generation\")\n",
    "    print(f\"   ✅ Explicit calibration examples in all prompts\")\n",
    "    print(f\"   ✅ Rater-specific bias and consistency parameters\")\n",
    "    print(f\"   ✅ Enhanced fallback simulation with realistic patterns\")\n",
    "    print(f\"   💰 COST CONTROL: Selective evaluation for individuals and experts\")\n",
    "    \n",
    "    # Step 1: Create enhanced rater profiles\n",
    "    print(f\"\\n📋 Step 1: Creating enhanced rater profiles...\")\n",
    "    all_raters, researchers, autism_individuals, autism_experts = create_all_rater_profiles()\n",
    "    print(f\"✅ Created {len(all_raters)} enhanced raters with individual characteristics:\")\n",
    "    print(f\"   - Researchers: {len(researchers)} (bias: -0.1 to +0.05, consistency: 0.85-0.88)\")\n",
    "    print(f\"   - Individuals with autism: {len(autism_individuals)} (bias: -0.125 to +0.125, consistency: 0.75-0.85)\")\n",
    "    print(f\"   - Autism experts: {len(autism_experts)} (bias: -0.1 to +0.1, consistency: 0.8-0.875)\")\n",
    "    \n",
    "    # Step 2: Load and prepare rating pairs\n",
    "    print(f\"\\n📊 Step 2: Loading and preparing rating pairs...\")\n",
    "    rating_pairs = load_and_prepare_rating_pairs(outgroup_csv_path, simulation_csv_path, debug)\n",
    "    \n",
    "    if not rating_pairs:\n",
    "        print(f\"❌ Failed to load rating pairs\")\n",
    "        return None, {}\n",
    "    \n",
    "    # Step 3: Execute cost-controlled LLM rating simulation\n",
    "    print(f\"\\n🎯 Step 3: Executing cost-controlled LLM rating simulation...\")\n",
    "    print(f\"💰 Cost Control Configuration:\")\n",
    "    print(f\"   - Individual sample size: {individual_sample_size} answers per person\")\n",
    "    print(f\"   - Expert sample size: {expert_sample_size} answers per person\")\n",
    "    print(f\"   - Based on paper methodology: experts evaluated limited subsets\")\n",
    "    \n",
    "    rating_results = execute_complete_rating_simulation(\n",
    "        rating_pairs, all_raters, researchers, autism_individuals, autism_experts, \n",
    "        model_name, individual_sample_size, expert_sample_size\n",
    "    )\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    print(f\"\\n💾 Step 4: Saving cost-controlled results...\")\n",
    "    saved_files = save_rating_results(rating_results, output_dir)\n",
    "    \n",
    "    print(f\"\\n🎉 Cost-Controlled LLM Rating Simulation Complete!\")\n",
    "    print(f\"📊 Final results: {len(rating_results)} pairs rated with cost optimization\")\n",
    "    print(f\"📁 Results saved to: {output_dir}\")\n",
    "    print(f\"\\n🔬 Expected Improvements:\")\n",
    "    print(f\"   📈 Krippendorff's α: 0.6-0.8 (significant improvement)\")\n",
    "    print(f\"   📈 ICC values: 0.7-0.9 (major reliability boost)\")\n",
    "    print(f\"   📈 Quality-based correlations between ratings\")\n",
    "    print(f\"   📈 Realistic rater variance with maintained consistency\")\n",
    "    print(f\"   💰 Cost optimization: ~76% reduction in LLM calls\")\n",
    "    \n",
    "    return rating_results, saved_files\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Your actual file paths\n",
    "    outgroup_path = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_original_answers\\out-group_answers.csv\"\n",
    "    simulation_path = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\out-group_simulated_answers\\gpt-3_5-turbo\\gpt-3.5-turbo_simulation_summary_20250708_115141.csv\"\n",
    "    \n",
    "    print(\"🚀 Starting COST-CONTROLLED LLM Rating Simulation System\")\n",
    "    print(\"=\"*65)\n",
    "    print(\"📊 Cost-Controlled Configuration:\")\n",
    "    print(\"   - Rating model: GPT-4o-mini with enhanced parameters\")\n",
    "    print(\"   - Temperature: 0.05-0.07 (ultra-low for consistency)\")\n",
    "    print(\"   - Quality-correlated rating generation\")\n",
    "    print(\"   - Calibration examples in all prompts\")\n",
    "    print(\"   - Individual rater characteristics\")\n",
    "    print(\"   - Enhanced fallback simulation\")\n",
    "    print(\"   - Researchers evaluate ALL answers\")\n",
    "    print(\"   💰 COST CONTROL:\")\n",
    "    print(\"   - Individuals: Each evaluates 50 randomly selected answers\")\n",
    "    print(\"   - Experts: Each evaluates 20 randomly selected answers\")\n",
    "    print(\"   - Expected cost reduction: ~76% (from ~$59 to ~$14)\")\n",
    "    print(\"   - Expected reliability: Krippendorff's α 0.6-0.8, ICC 0.7-0.9\")\n",
    "    \n",
    "    # Run cost-controlled simulation\n",
    "    results, saved_files = run_complete_llm_rating_simulation(\n",
    "        outgroup_csv_path=outgroup_path,\n",
    "        simulation_csv_path=simulation_path,\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        output_dir=r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\",\n",
    "        individual_sample_size=50,  # 每个个体评估50个答案\n",
    "        expert_sample_size=20       # 每个专家评估20个答案\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(f\"✅ Successfully generated {len(results)} cost-controlled rating results\")\n",
    "        print(f\"📁 Files saved to: rating_results/\")\n",
    "        \n",
    "        # Show data distribution\n",
    "        sources = {}\n",
    "        for result in results:\n",
    "            source = result.get('source', 'unknown')\n",
    "            sources[source] = sources.get(source, 0) + 1\n",
    "        \n",
    "        print(f\"📊 Cost-Controlled Rating Results Distribution:\")\n",
    "        for source, count in sources.items():\n",
    "            print(f\"   {source}: {count}\")\n",
    "            \n",
    "        # Show cost and time estimates\n",
    "        total_pairs = len(results)\n",
    "        total_calls = 400 + (50 * 6) + (20 * 11)  # 研究者 + 个体 + 专家\n",
    "        estimated_cost = total_calls * 0.0015\n",
    "        estimated_time = total_calls * 0.4 / 60  # minutes\n",
    "        original_cost = (total_pairs * 19) * 0.0015\n",
    "        savings = original_cost - estimated_cost\n",
    "        \n",
    "        print(f\"📊 Cost-Controlled Run Statistics:\")\n",
    "        print(f\"   Total LLM calls: {total_calls} (vs original {total_pairs * 19})\")\n",
    "        print(f\"   Actual cost: ${estimated_cost:.2f} (vs original ~${original_cost:.2f})\")\n",
    "        print(f\"   Cost savings: ${savings:.2f} ({(savings/original_cost*100):.1f}% reduction)\")\n",
    "        print(f\"   Estimated time: {estimated_time:.1f} minutes\")\n",
    "        print(f\"   🎯 Run the reliability analysis to see improvements!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Cost-controlled rating simulation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc616501-52a8-40c6-83d4-700597a583d4",
   "metadata": {},
   "source": [
    "### 5. Rating Results Analysis\n",
    "#### 5.1 Assessing Raters Agreement - Calculate Krippendorff's α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6d108d2-a00d-4026-a958-383191e7827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting Krippendorff's α Analysis Only\n",
      "==================================================\n",
      "📊 Analyzing researcher agreement on Yes/No measures\n",
      "✅ Successfully loaded cost-controlled data: 200 rows, 50 columns\n",
      "\n",
      "📊 Data Structure Check:\n",
      "   Total answer pairs: 200\n",
      "   Researcher columns: 10\n",
      "\n",
      "🔍 Researcher Evaluation Coverage Analysis:\n",
      "==================================================\n",
      "\n",
      "📊 Researcher Coverage:\n",
      "  R1: 200/200 (100.0%)\n",
      "  R2: 200/200 (100.0%)\n",
      "\n",
      "==============================\n",
      "📊 Calculating Krippendorff's α\n",
      "==============================\n",
      "\n",
      "Calculating Krippendorff's α for Directness...\n",
      "  α = 0.735 (Good)\n",
      "\n",
      "Calculating Krippendorff's α for Additional_Information...\n",
      "  α = 0.755 (Good)\n",
      "\n",
      "Calculating Krippendorff's α for Informational_Support...\n",
      "  α = 0.887 (Excellent)\n",
      "\n",
      "Calculating Krippendorff's α for Emotional_Support...\n",
      "  α = 0.612 (Good)\n",
      "\n",
      "==============================\n",
      "💾 Saving Krippendorff's α Results\n",
      "==============================\n",
      "\n",
      "✅ Cost-controlled Krippendorff's α results saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\cost_controlled_krippendorff_alpha_results.csv\n",
      "✅ Researcher coverage statistics saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\researcher_coverage_statistics.csv\n",
      "\n",
      "==============================\n",
      "📋 Analysis Summary\n",
      "==============================\n",
      "\n",
      "🎯 Krippendorff's α Results Summary:\n",
      "  Directness: α = 0.735 (Good)\n",
      "    [n_raters=2, n_items=200]\n",
      "  Additional_Information: α = 0.755 (Good)\n",
      "    [n_raters=2, n_items=200]\n",
      "  Informational_Support: α = 0.887 (Excellent)\n",
      "    [n_raters=2, n_items=200]\n",
      "  Emotional_Support: α = 0.612 (Good)\n",
      "    [n_raters=2, n_items=200]\n",
      "\n",
      "🎯 Researcher Coverage Summary:\n",
      "  Total researcher evaluations: 400\n",
      "  Expected evaluations: 400\n",
      "  Coverage rate: 100.0%\n",
      "\n",
      "📊 Analysis Quality Assessment:\n",
      "  Successful α calculations: 4/4\n",
      "  ✅ All α calculations completed successfully\n",
      "\n",
      "🎯 Results vs. Expected Ranges:\n",
      "  Expected Krippendorff's α: 0.6-0.8 (Good range)\n",
      "  Alpha values in expected range (0.6-0.8): 3/4\n",
      "  Alpha values excellent (≥0.8): 1/4\n",
      "  Alpha values below good (<0.6): 0/4\n",
      "\n",
      "📊 Detailed Results Breakdown:\n",
      "  Excellent reliability (α ≥ 0.8): Informational_Support\n",
      "  Good reliability (0.6 ≤ α < 0.8): Directness, Additional_Information, Emotional_Support\n",
      "\n",
      "✅ Krippendorff's α analysis completed!\n",
      "📁 Result files saved in: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\n",
      "💡 Note: Analysis focused on researcher agreement for Yes/No measures\n",
      "🎯 Researchers maintained 100% evaluation coverage in cost-controlled design\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# COST-CONTROLLED KRIPPENDORFF'S α ANALYSIS ONLY\n",
    "# Analysis for Researcher Agreement on Yes/No measures\n",
    "# ===================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import krippendorff\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_krippendorff_alpha_cost_controlled(data_df, measures, raters):\n",
    "    \"\"\"\n",
    "    Calculate Krippendorff's α for Yes/No measures (Researchers only)\n",
    "    Cost-controlled version - researchers still evaluate all answers\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for measure in measures:\n",
    "        print(f\"\\nCalculating Krippendorff's α for {measure}...\")\n",
    "        \n",
    "        # Build rater matrix for researchers (they evaluate all answers)\n",
    "        rater_data = []\n",
    "        for rater in raters:\n",
    "            col_name = f'Researcher_{rater}_{measure}'\n",
    "            if col_name in data_df.columns:\n",
    "                values = data_df[col_name].values\n",
    "                rater_data.append(values)\n",
    "        \n",
    "        if len(rater_data) >= 2:\n",
    "            # Convert to format required by krippendorff package\n",
    "            reliability_data = np.array(rater_data)\n",
    "            \n",
    "            try:\n",
    "                alpha = krippendorff.alpha(reliability_data, level_of_measurement='ordinal')\n",
    "                results[measure] = {\n",
    "                    'alpha': alpha,\n",
    "                    'raters': len(rater_data),\n",
    "                    'items': len(values) if len(rater_data) > 0 else 0,\n",
    "                    'interpretation': interpret_alpha(alpha)\n",
    "                }\n",
    "                print(f\"  α = {alpha:.3f} ({interpret_alpha(alpha)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Calculation failed: {e}\")\n",
    "                results[measure] = {\n",
    "                    'alpha': np.nan,\n",
    "                    'raters': len(rater_data),\n",
    "                    'items': len(values) if len(rater_data) > 0 else 0,\n",
    "                    'interpretation': 'calculation_failed'\n",
    "                }\n",
    "        else:\n",
    "            print(f\"  Insufficient data, need at least 2 raters\")\n",
    "            results[measure] = {\n",
    "                'alpha': np.nan,\n",
    "                'raters': len(rater_data),\n",
    "                'items': 0,\n",
    "                'interpretation': 'insufficient_data'\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def interpret_alpha(alpha):\n",
    "    \"\"\"Interpret Krippendorff's α values\"\"\"\n",
    "    if np.isnan(alpha):\n",
    "        return \"Cannot calculate\"\n",
    "    elif alpha < 0.20:\n",
    "        return \"Poor\"\n",
    "    elif alpha < 0.40:\n",
    "        return \"Fair\"\n",
    "    elif alpha < 0.60:\n",
    "        return \"Moderate\"\n",
    "    elif alpha < 0.80:\n",
    "        return \"Good\"\n",
    "    else:\n",
    "        return \"Excellent\"\n",
    "\n",
    "def analyze_researcher_coverage(data_df):\n",
    "    \"\"\"Analyze researcher evaluation coverage\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 Researcher Evaluation Coverage Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    coverage_stats = {}\n",
    "    \n",
    "    # Researcher coverage (should be 100%)\n",
    "    researcher_raters = ['R1', 'R2']\n",
    "    print(f\"\\n📊 Researcher Coverage:\")\n",
    "    \n",
    "    for rater in researcher_raters:\n",
    "        helpfulness_col = f'Researcher_{rater}_Helpfulness'\n",
    "        if helpfulness_col in data_df.columns:\n",
    "            valid_ratings = data_df[helpfulness_col].notna().sum()\n",
    "            coverage_stats[f'Researcher_{rater}'] = {\n",
    "                'evaluated': valid_ratings,\n",
    "                'total': len(data_df),\n",
    "                'coverage_pct': valid_ratings / len(data_df) * 100\n",
    "            }\n",
    "            print(f\"  {rater}: {valid_ratings}/{len(data_df)} ({valid_ratings/len(data_df)*100:.1f}%)\")\n",
    "    \n",
    "    return coverage_stats\n",
    "\n",
    "def save_krippendorff_alpha_results(alpha_results, coverage_stats, output_dir):\n",
    "    \"\"\"Save Krippendorff's α analysis results\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save Krippendorff's α results\n",
    "    alpha_df_data = []\n",
    "    for measure, result in alpha_results.items():\n",
    "        alpha_df_data.append({\n",
    "            'Measure': measure,\n",
    "            'Krippendorff_Alpha': result['alpha'],\n",
    "            'N_Raters': result['raters'],\n",
    "            'N_Items': result['items'],\n",
    "            'Interpretation': result['interpretation']\n",
    "        })\n",
    "    \n",
    "    alpha_df = pd.DataFrame(alpha_df_data)\n",
    "    alpha_file = os.path.join(output_dir, 'cost_controlled_krippendorff_alpha_results.csv')\n",
    "    alpha_df.to_csv(alpha_file, index=False, encoding='utf-8')\n",
    "    print(f\"\\n✅ Cost-controlled Krippendorff's α results saved to: {alpha_file}\")\n",
    "    \n",
    "    # Save coverage statistics\n",
    "    coverage_df_data = []\n",
    "    for rater, stats in coverage_stats.items():\n",
    "        coverage_df_data.append({\n",
    "            'Rater': rater,\n",
    "            'Evaluated': stats['evaluated'],\n",
    "            'Total': stats['total'],\n",
    "            'Coverage_Percentage': stats['coverage_pct']\n",
    "        })\n",
    "    \n",
    "    coverage_df = pd.DataFrame(coverage_df_data)\n",
    "    coverage_file = os.path.join(output_dir, 'researcher_coverage_statistics.csv')\n",
    "    coverage_df.to_csv(coverage_file, index=False, encoding='utf-8')\n",
    "    print(f\"✅ Researcher coverage statistics saved to: {coverage_file}\")\n",
    "    \n",
    "    return alpha_file, coverage_file\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function - Execute Krippendorff's α analysis only\"\"\"\n",
    "    \n",
    "    # Data file path for cost-controlled results\n",
    "    # NEW INPUT RATING\n",
    "    csv_path = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\cost_controlled_llm_rating_results_20250724_121156.csv\"\n",
    "    output_dir = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\"\n",
    "    \n",
    "    print(\"🔍 Starting Krippendorff's α Analysis Only\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"📊 Analyzing researcher agreement on Yes/No measures\")\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        data_df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        print(f\"✅ Successfully loaded cost-controlled data: {len(data_df)} rows, {len(data_df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check data structure\n",
    "    print(f\"\\n📊 Data Structure Check:\")\n",
    "    print(f\"   Total answer pairs: {len(data_df)}\")\n",
    "    researcher_cols = [col for col in data_df.columns if col.startswith('Researcher_')]\n",
    "    print(f\"   Researcher columns: {len(researcher_cols)}\")\n",
    "    \n",
    "    # Analyze researcher coverage\n",
    "    coverage_stats = analyze_researcher_coverage(data_df)\n",
    "    \n",
    "    # Calculate Krippendorff's α (Researchers only)\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"📊 Calculating Krippendorff's α\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    yes_no_measures = ['Directness', 'Additional_Information', 'Informational_Support', 'Emotional_Support']\n",
    "    researcher_raters = ['R1', 'R2']\n",
    "    \n",
    "    alpha_results = calculate_krippendorff_alpha_cost_controlled(data_df, yes_no_measures, researcher_raters)\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"💾 Saving Krippendorff's α Results\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    alpha_file, coverage_file = save_krippendorff_alpha_results(alpha_results, coverage_stats, output_dir)\n",
    "    \n",
    "    # Summary report\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(\"📋 Analysis Summary\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Krippendorff's α Results Summary:\")\n",
    "    for measure, result in alpha_results.items():\n",
    "        alpha_val = result['alpha']\n",
    "        interpretation = result['interpretation']\n",
    "        n_raters = result['raters']\n",
    "        n_items = result['items']\n",
    "        \n",
    "        if not np.isnan(alpha_val):\n",
    "            print(f\"  {measure}: α = {alpha_val:.3f} ({interpretation})\")\n",
    "            print(f\"    [n_raters={n_raters}, n_items={n_items}]\")\n",
    "        else:\n",
    "            print(f\"  {measure}: Calculation failed ({interpretation})\")\n",
    "    \n",
    "    # Coverage summary\n",
    "    print(f\"\\n🎯 Researcher Coverage Summary:\")\n",
    "    total_researcher_evaluations = sum(stats['evaluated'] for stats in coverage_stats.values())\n",
    "    expected_evaluations = len(data_df) * len(researcher_raters)\n",
    "    \n",
    "    print(f\"  Total researcher evaluations: {total_researcher_evaluations}\")\n",
    "    print(f\"  Expected evaluations: {expected_evaluations}\")\n",
    "    print(f\"  Coverage rate: {total_researcher_evaluations/expected_evaluations*100:.1f}%\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    successful_alpha = sum(1 for result in alpha_results.values() if not np.isnan(result['alpha']))\n",
    "    total_alpha = len(alpha_results)\n",
    "    \n",
    "    print(f\"\\n📊 Analysis Quality Assessment:\")\n",
    "    print(f\"  Successful α calculations: {successful_alpha}/{total_alpha}\")\n",
    "    \n",
    "    if successful_alpha == total_alpha:\n",
    "        print(f\"  ✅ All α calculations completed successfully\")\n",
    "    elif successful_alpha >= total_alpha * 0.75:\n",
    "        print(f\"  ✅ Most α calculations completed successfully\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  Some α calculations failed\")\n",
    "    \n",
    "    # Comparison with expected ranges\n",
    "    print(f\"\\n🎯 Results vs. Expected Ranges:\")\n",
    "    print(f\"  Expected Krippendorff's α: 0.6-0.8 (Good range)\")\n",
    "    \n",
    "    alpha_in_range = sum(1 for result in alpha_results.values() \n",
    "                        if not np.isnan(result['alpha']) and 0.6 <= result['alpha'] <= 0.8)\n",
    "    alpha_excellent = sum(1 for result in alpha_results.values() \n",
    "                         if not np.isnan(result['alpha']) and result['alpha'] >= 0.8)\n",
    "    alpha_below_good = sum(1 for result in alpha_results.values() \n",
    "                          if not np.isnan(result['alpha']) and result['alpha'] < 0.6)\n",
    "    \n",
    "    print(f\"  Alpha values in expected range (0.6-0.8): {alpha_in_range}/{successful_alpha}\")\n",
    "    print(f\"  Alpha values excellent (≥0.8): {alpha_excellent}/{successful_alpha}\")\n",
    "    print(f\"  Alpha values below good (<0.6): {alpha_below_good}/{successful_alpha}\")\n",
    "    \n",
    "    # Detailed results breakdown\n",
    "    print(f\"\\n📊 Detailed Results Breakdown:\")\n",
    "    excellent_measures = [measure for measure, result in alpha_results.items() \n",
    "                         if not np.isnan(result['alpha']) and result['alpha'] >= 0.8]\n",
    "    good_measures = [measure for measure, result in alpha_results.items() \n",
    "                    if not np.isnan(result['alpha']) and 0.6 <= result['alpha'] < 0.8]\n",
    "    \n",
    "    if excellent_measures:\n",
    "        print(f\"  Excellent reliability (α ≥ 0.8): {', '.join(excellent_measures)}\")\n",
    "    if good_measures:\n",
    "        print(f\"  Good reliability (0.6 ≤ α < 0.8): {', '.join(good_measures)}\")\n",
    "    \n",
    "    print(f\"\\n✅ Krippendorff's α analysis completed!\")\n",
    "    print(f\"📁 Result files saved in: {output_dir}\")\n",
    "    print(f\"💡 Note: Analysis focused on researcher agreement for Yes/No measures\")\n",
    "    print(f\"🎯 Researchers maintained 100% evaluation coverage in cost-controlled design\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959123bd-bc91-409a-bb30-2385c235569a",
   "metadata": {},
   "source": [
    "#### 5.2 out-group vs. ai-simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49bff2f7-0030-4331-978a-60d25a091d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cost Controlled Table 3 Analysis...\n",
      "============================================================\n",
      "Total records: 200\n",
      "Sources: source\n",
      "ai_generated       100\n",
      "out_group_human    100\n",
      "Name: count, dtype: int64\n",
      "AI-generated (In-group) records: 100\n",
      "Out-group human records: 100\n",
      "\n",
      "Processing Directness...\n",
      "  AI (In-group) valid samples: 100\n",
      "  Out-group valid samples: 100\n",
      "  AI (In-group): μ=0.69, median=1.00\n",
      "  Out-group: μ=0.48, median=0.50\n",
      "  Mann-Whitney U: z=2.64, p=0.003498\n",
      "\n",
      "Processing Additional_Information...\n",
      "  AI (In-group) valid samples: 100\n",
      "  Out-group valid samples: 100\n",
      "  AI (In-group): μ=0.97, median=1.00\n",
      "  Out-group: μ=0.79, median=1.00\n",
      "  Mann-Whitney U: z=2.82, p=0.000007\n",
      "\n",
      "Processing Informational_Support...\n",
      "  AI (In-group) valid samples: 100\n",
      "  Out-group valid samples: 100\n",
      "  AI (In-group): μ=0.78, median=1.00\n",
      "  Out-group: μ=0.41, median=0.00\n",
      "  Mann-Whitney U: z=4.81, p=0.000000\n",
      "\n",
      "Processing Emotional_Support...\n",
      "  AI (In-group) valid samples: 100\n",
      "  Out-group valid samples: 100\n",
      "  AI (In-group): μ=0.59, median=0.50\n",
      "  Out-group: μ=0.27, median=0.00\n",
      "  Mann-Whitney U: z=4.79, p=0.000000\n",
      "\n",
      "Processing Helpfulness...\n",
      "  AI (In-group) valid samples: 100\n",
      "  Out-group valid samples: 100\n",
      "  AI (In-group): μ=3.35, median=3.75\n",
      "  Out-group: μ=2.20, median=1.00\n",
      "  Mann-Whitney U: z=4.66, p=0.000001\n",
      "\n",
      "================================================================================\n",
      "TABLE 3: Descriptive statistics comparing AI-generated and out-group answers\n",
      "================================================================================\n",
      "Answer source Mean (μ)   Median   SD (σ)   Min      Max     \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Directness (z=2.64, p=0.003498)\n",
      "--------------------------------------------------\n",
      "In-group     0.69       1.00     0.41     0.00     1.00    \n",
      "Out-group    0.48       0.50     0.48     0.00     1.00    \n",
      "\n",
      "Additional_Information (z=2.82, p=0.000007)\n",
      "--------------------------------------------------\n",
      "In-group     0.97       1.00     0.16     0.00     1.00    \n",
      "Out-group    0.79       1.00     0.38     0.00     1.00    \n",
      "\n",
      "Informational_Support (z=4.81, p=0.000000)\n",
      "--------------------------------------------------\n",
      "In-group     0.78       1.00     0.40     0.00     1.00    \n",
      "Out-group    0.41       0.00     0.48     0.00     1.00    \n",
      "\n",
      "Emotional_Support (z=4.79, p=0.000000)\n",
      "--------------------------------------------------\n",
      "In-group     0.59       0.50     0.43     0.00     1.00    \n",
      "Out-group    0.27       0.00     0.39     0.00     1.00    \n",
      "\n",
      "Helpfulness (z=4.66, p=0.000001)\n",
      "--------------------------------------------------\n",
      "In-group     3.35       3.75     1.64     1.00     5.00    \n",
      "Out-group    2.20       1.00     1.48     1.00     5.00    \n",
      "\n",
      "Table 3 saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\\Table3_ai_vs_outgroup.csv\n",
      "\n",
      "Table 3. Descriptive statistics for the five outcome measures\n",
      "comparing In-group (N=5) and out-group (N=5) answers for\n",
      "cost controlled questions. The z statistic is the Mann-Whitney U score.\n",
      "\n",
      "Analysis completed! Generated Table 3 format comparing:\n",
      "- In-group: AI-generated responses\n",
      "- Out-group: Out-group human responses\n",
      "\n",
      "Output file: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728/Table3_ai_vs_outgroup.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_cost_controlled_table3(file_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Analyze cost controlled rating data and generate Table 3 format comparing AI-generated vs out-group\n",
    "    \"\"\"\n",
    "    # Create output directory if specified\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Sources: {df['source'].value_counts()}\")\n",
    "    \n",
    "    # Separate groups (Note: using 'ai_generated' as 'In-group' and 'out_group_human' as 'Out-group')\n",
    "    ai_data = df[df['source'] == 'ai_generated'].copy()  # This will be \"In-group\" in table\n",
    "    out_group_data = df[df['source'] == 'out_group_human'].copy()  # This will be \"Out-group\" in table\n",
    "    \n",
    "    print(f\"AI-generated (In-group) records: {len(ai_data)}\")\n",
    "    print(f\"Out-group human records: {len(out_group_data)}\")\n",
    "    \n",
    "    # Define measures\n",
    "    measures = [\n",
    "        'Directness', \n",
    "        'Additional_Information', \n",
    "        'Informational_Support', \n",
    "        'Emotional_Support', \n",
    "        'Helpfulness'\n",
    "    ]\n",
    "    \n",
    "    # Store results for Table 3\n",
    "    table_data = []\n",
    "    \n",
    "    # Process each measure\n",
    "    for measure in measures:\n",
    "        print(f\"\\nProcessing {measure}...\")\n",
    "        \n",
    "        # Column names for R1 and R2\n",
    "        r1_col = f'Researcher_R1_{measure}'\n",
    "        r2_col = f'Researcher_R2_{measure}'\n",
    "        \n",
    "        # Check if columns exist\n",
    "        if r1_col not in df.columns or r2_col not in df.columns:\n",
    "            print(f\"Warning: Missing columns for {measure}\")\n",
    "            continue\n",
    "        \n",
    "        # Process AI data (In-group)\n",
    "        ai_r1 = pd.to_numeric(ai_data[r1_col], errors='coerce')\n",
    "        ai_r2 = pd.to_numeric(ai_data[r2_col], errors='coerce')\n",
    "        ai_avg = (ai_r1 + ai_r2) / 2\n",
    "        ai_clean = ai_avg.dropna()\n",
    "        \n",
    "        # Process Out-group data\n",
    "        out_r1 = pd.to_numeric(out_group_data[r1_col], errors='coerce')\n",
    "        out_r2 = pd.to_numeric(out_group_data[r2_col], errors='coerce')\n",
    "        out_avg = (out_r1 + out_r2) / 2\n",
    "        out_clean = out_avg.dropna()\n",
    "        \n",
    "        print(f\"  AI (In-group) valid samples: {len(ai_clean)}\")\n",
    "        print(f\"  Out-group valid samples: {len(out_clean)}\")\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(ai_clean) < 2 or len(out_clean) < 2:\n",
    "            print(f\"  Skipping {measure} due to insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        try:\n",
    "            statistic, p_value = mannwhitneyu(ai_clean, out_clean, alternative='two-sided')\n",
    "            \n",
    "            # Calculate z-score approximation\n",
    "            n1, n2 = len(ai_clean), len(out_clean)\n",
    "            mean_u = n1 * n2 / 2\n",
    "            std_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "            z_score = (statistic - mean_u) / std_u\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Statistical test failed for {measure}: {e}\")\n",
    "            z_score, p_value = 0, 1\n",
    "        \n",
    "        # Store statistics for both groups\n",
    "        ai_stats = {\n",
    "            'mean': ai_clean.mean(),\n",
    "            'median': ai_clean.median(),\n",
    "            'std': ai_clean.std(),\n",
    "            'min': ai_clean.min(),\n",
    "            'max': ai_clean.max()\n",
    "        }\n",
    "        \n",
    "        out_stats = {\n",
    "            'mean': out_clean.mean(),\n",
    "            'median': out_clean.median(),\n",
    "            'std': out_clean.std(),\n",
    "            'min': out_clean.min(),\n",
    "            'max': out_clean.max()\n",
    "        }\n",
    "        \n",
    "        print(f\"  AI (In-group): μ={ai_stats['mean']:.2f}, median={ai_stats['median']:.2f}\")\n",
    "        print(f\"  Out-group: μ={out_stats['mean']:.2f}, median={out_stats['median']:.2f}\")\n",
    "        print(f\"  Mann-Whitney U: z={z_score:.2f}, p={p_value:.6f}\")\n",
    "        \n",
    "        # Add to table data\n",
    "        table_data.append({\n",
    "            'measure': measure,\n",
    "            'z_score': z_score,\n",
    "            'p_value': p_value,\n",
    "            'ai_stats': ai_stats,\n",
    "            'out_stats': out_stats\n",
    "        })\n",
    "    \n",
    "    # Create Table 3 format\n",
    "    create_table3_format(table_data, output_dir)\n",
    "    \n",
    "    return table_data\n",
    "\n",
    "def create_table3_format(table_data, output_dir):\n",
    "    \"\"\"Create Table 3 in the exact format as shown in the image\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 3: Descriptive statistics comparing AI-generated and out-group answers\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print header\n",
    "    print(f\"{'Answer source':<12} {'Mean (μ)':<10} {'Median':<8} {'SD (σ)':<8} {'Min':<8} {'Max':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    table_rows = []\n",
    "    \n",
    "    for data in table_data:\n",
    "        measure = data['measure']\n",
    "        z_score = data['z_score']\n",
    "        p_value = data['p_value']\n",
    "        ai_stats = data['ai_stats']\n",
    "        out_stats = data['out_stats']\n",
    "        \n",
    "        # Print measure header with statistics\n",
    "        print(f\"\\n{measure} (z={z_score:.2f}, p={p_value:.6f})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # In-group (AI-generated) row\n",
    "        print(f\"{'In-group':<12} {ai_stats['mean']:<10.2f} {ai_stats['median']:<8.2f} {ai_stats['std']:<8.2f} {ai_stats['min']:<8.2f} {ai_stats['max']:<8.2f}\")\n",
    "        \n",
    "        # Out-group row  \n",
    "        print(f\"{'Out-group':<12} {out_stats['mean']:<10.2f} {out_stats['median']:<8.2f} {out_stats['std']:<8.2f} {out_stats['min']:<8.2f} {out_stats['max']:<8.2f}\")\n",
    "        \n",
    "        # Store for CSV output\n",
    "        table_rows.extend([\n",
    "            {\n",
    "                'Answer_source': 'In-group',\n",
    "                'Measure': f\"{measure} (z={z_score:.2f}, p={p_value:.6f})\",\n",
    "                'Mean_μ': round(ai_stats['mean'], 2),\n",
    "                'Median': round(ai_stats['median'], 2),\n",
    "                'SD_σ': round(ai_stats['std'], 2),\n",
    "                'Min': round(ai_stats['min'], 2),\n",
    "                'Max': round(ai_stats['max'], 2)\n",
    "            },\n",
    "            {\n",
    "                'Answer_source': 'Out-group',\n",
    "                'Measure': '',  # Empty for second row\n",
    "                'Mean_μ': round(out_stats['mean'], 2),\n",
    "                'Median': round(out_stats['median'], 2),\n",
    "                'SD_σ': round(out_stats['std'], 2),\n",
    "                'Min': round(out_stats['min'], 2),\n",
    "                'Max': round(out_stats['max'], 2)\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Save to CSV if output directory specified\n",
    "    if output_dir:\n",
    "        table_df = pd.DataFrame(table_rows)\n",
    "        csv_path = os.path.join(output_dir, 'Table3_ai_vs_outgroup.csv')\n",
    "        table_df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nTable 3 saved to: {csv_path}\")\n",
    "    \n",
    "    print(f\"\\nTable 3. Descriptive statistics for the five outcome measures\")\n",
    "    print(f\"comparing In-group (N={len(table_data)}) and out-group (N={len(table_data)}) answers for\")\n",
    "    print(f\"cost controlled questions. The z statistic is the Mann-Whitney U score.\")\n",
    "\n",
    "# Main execution function\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    # NEW INPUT RATING\n",
    "    input_file = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\\cost_controlled_llm_rating_results_20250728_003329.csv\"\n",
    "    output_directory = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\"  # Optional: set to None if you don't want to save files\n",
    "    \n",
    "    # Run the analysis\n",
    "    print(\"Starting Cost Controlled Table 3 Analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = analyze_cost_controlled_table3(input_file, output_directory)\n",
    "    \n",
    "    print(f\"\\nAnalysis completed! Generated Table 3 format comparing:\")\n",
    "    print(\"- In-group: AI-generated responses\")\n",
    "    print(\"- Out-group: Out-group human responses\")\n",
    "    \n",
    "    if output_directory:\n",
    "        print(f\"\\nOutput file: {output_directory}/Table3_ai_vs_outgroup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e2857-b97b-4eae-a222-97d816980f14",
   "metadata": {},
   "source": [
    "#### 5.3 In-group vs Out-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47093028-7e9d-4ef0-ab25-028b8bd8da29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Complete In-group vs Out-group Analysis...\n",
      "============================================================\n",
      "=====================================================================================\n",
      "Complete In-group vs Out-group Analysis with Mann-Whitney U Test\n",
      "=====================================================================================\n",
      "1. Reading in-group data...\n",
      "   In-group human records: 100\n",
      "\n",
      "2. Reading out-group source data...\n",
      "   Out-group human records: 100\n",
      "\n",
      "3. Processing 5 measures...\n",
      "\n",
      "   Processing Directness...\n",
      "     In-group valid samples: 100\n",
      "     Out-group valid samples: 100\n",
      "     In-group: μ=0.36, median=0.00, n=100\n",
      "     Out-group: μ=0.48, median=0.50, n=100\n",
      "     Mann-Whitney U: z=-1.55, p=0.083979 ns\n",
      "\n",
      "   Processing Additional_Information...\n",
      "     In-group valid samples: 100\n",
      "     Out-group valid samples: 100\n",
      "     In-group: μ=0.87, median=1.00, n=100\n",
      "     Out-group: μ=0.79, median=1.00, n=100\n",
      "     Mann-Whitney U: z=1.19, p=0.103750 ns\n",
      "\n",
      "   Processing Informational_Support...\n",
      "     In-group valid samples: 100\n",
      "     Out-group valid samples: 100\n",
      "     In-group: μ=0.27, median=0.00, n=100\n",
      "     Out-group: μ=0.41, median=0.00, n=100\n",
      "     Mann-Whitney U: z=-1.71, p=0.047738 *\n",
      "\n",
      "   Processing Emotional_Support...\n",
      "     In-group valid samples: 100\n",
      "     Out-group valid samples: 100\n",
      "     In-group: μ=0.25, median=0.00, n=100\n",
      "     Out-group: μ=0.27, median=0.00, n=100\n",
      "     Mann-Whitney U: z=-0.24, p=0.772910 ns\n",
      "\n",
      "   Processing Helpfulness...\n",
      "     In-group valid samples: 100\n",
      "     Out-group valid samples: 100\n",
      "     In-group: μ=1.86, median=1.00, n=100\n",
      "     Out-group: μ=2.20, median=1.00, n=100\n",
      "     Mann-Whitney U: z=-1.42, p=0.114772 ns\n",
      "\n",
      "=====================================================================================\n",
      "TABLE 3: Descriptive statistics comparing in-group and out-group answers\n",
      "=====================================================================================\n",
      "\n",
      "Directness (z=-1.55, p=0.083979 ns)\n",
      "-----------------------------------------------------------------\n",
      "Answer source   Mean (μ)   Median   SD (σ)   Min      Max      N       \n",
      "--------------------------------------------------------------------------------\n",
      "In-group        0.36       0.00     0.44     0.00     1.00     100     \n",
      "Out-group       0.48       0.50     0.48     0.00     1.00     100     \n",
      "\n",
      "Additional_Information (z=1.19, p=0.103750 ns)\n",
      "-----------------------------------------------------------------\n",
      "Answer source   Mean (μ)   Median   SD (σ)   Min      Max      N       \n",
      "--------------------------------------------------------------------------------\n",
      "In-group        0.87       1.00     0.30     0.00     1.00     100     \n",
      "Out-group       0.79       1.00     0.38     0.00     1.00     100     \n",
      "\n",
      "Informational_Support (z=-1.71, p=0.047738 *)\n",
      "-----------------------------------------------------------------\n",
      "Answer source   Mean (μ)   Median   SD (σ)   Min      Max      N       \n",
      "--------------------------------------------------------------------------------\n",
      "In-group        0.27       0.00     0.41     0.00     1.00     100     \n",
      "Out-group       0.41       0.00     0.48     0.00     1.00     100     \n",
      "\n",
      "Emotional_Support (z=-0.24, p=0.772910 ns)\n",
      "-----------------------------------------------------------------\n",
      "Answer source   Mean (μ)   Median   SD (σ)   Min      Max      N       \n",
      "--------------------------------------------------------------------------------\n",
      "In-group        0.25       0.00     0.39     0.00     1.00     100     \n",
      "Out-group       0.27       0.00     0.39     0.00     1.00     100     \n",
      "\n",
      "Helpfulness (z=-1.42, p=0.114772 ns)\n",
      "-----------------------------------------------------------------\n",
      "Answer source   Mean (μ)   Median   SD (σ)   Min      Max      N       \n",
      "--------------------------------------------------------------------------------\n",
      "In-group        1.86       1.00     1.32     1.00     5.00     100     \n",
      "Out-group       2.20       1.00     1.48     1.00     5.00     100     \n",
      "\n",
      "=====================================================================================\n",
      "SUMMARY STATISTICS\n",
      "=====================================================================================\n",
      "Total measures analyzed: 5\n",
      "Statistically significant differences (p < 0.05): 1\n",
      "In-group sample size: 100\n",
      "Out-group sample size: 100\n",
      "\n",
      "Table 3. Descriptive statistics for the five outcome measures\n",
      "comparing In-group (N=100) and out-group (N=100) answers for\n",
      "cost controlled questions. The z statistic is the Mann-Whitney U score.\n",
      "Significance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\n",
      "\n",
      "Table 3 results saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\\Table3_ingroup_vs_outgroup.csv\n",
      "Detailed statistical results saved to: D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\\Table3_ingroup_vs_outgroup.csv\n",
      "\n",
      "Analysis completed successfully!\n",
      "Generated files:\n",
      "- Table3_ingroup_vs_outgroup.csv\n",
      "- Table3_ingroup_vs_outgroup.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_ingroup_vs_outgroup_complete(ingroup_file, outgroup_source_file, output_dir=None):\n",
    "    \"\"\"\n",
    "    Complete analysis comparing in-group human vs out-group human data\n",
    "    \n",
    "    Parameters:\n",
    "    - ingroup_file: CSV file containing in_group_human data\n",
    "    - outgroup_source_file: CSV file containing out_group_human data (to recalculate statistics)\n",
    "    - output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*85)\n",
    "    print(\"Complete In-group vs Out-group Analysis with Mann-Whitney U Test\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    # Read in-group data\n",
    "    print(\"1. Reading in-group data...\")\n",
    "    df_ingroup = pd.read_csv(ingroup_file)\n",
    "    in_group_data = df_ingroup[df_ingroup['source'] == 'in_group_human'].copy()\n",
    "    print(f\"   In-group human records: {len(in_group_data)}\")\n",
    "    \n",
    "    # Read out-group source data to recalculate statistics\n",
    "    print(\"\\n2. Reading out-group source data...\")\n",
    "    df_outgroup_source = pd.read_csv(outgroup_source_file)\n",
    "    out_group_data = df_outgroup_source[df_outgroup_source['source'] == 'out_group_human'].copy()\n",
    "    print(f\"   Out-group human records: {len(out_group_data)}\")\n",
    "    \n",
    "    if len(out_group_data) == 0:\n",
    "        print(\"   Warning: No out_group_human data found in source file\")\n",
    "        return None\n",
    "    \n",
    "    # Define measures\n",
    "    measures = [\n",
    "        'Directness', \n",
    "        'Additional_Information', \n",
    "        'Informational_Support', \n",
    "        'Emotional_Support', \n",
    "        'Helpfulness'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n3. Processing {len(measures)} measures...\")\n",
    "    \n",
    "    # Store results for Table 3\n",
    "    table_data = []\n",
    "    table_rows = []\n",
    "    \n",
    "    # Process each measure\n",
    "    for measure in measures:\n",
    "        print(f\"\\n   Processing {measure}...\")\n",
    "        \n",
    "        # Column names for R1 and R2\n",
    "        r1_col = f'Researcher_R1_{measure}'\n",
    "        r2_col = f'Researcher_R2_{measure}'\n",
    "        \n",
    "        # Check if columns exist in both datasets\n",
    "        in_missing = r1_col not in df_ingroup.columns or r2_col not in df_ingroup.columns\n",
    "        out_missing = r1_col not in df_outgroup_source.columns or r2_col not in df_outgroup_source.columns\n",
    "        \n",
    "        if in_missing or out_missing:\n",
    "            print(f\"     Warning: Missing columns for {measure}\")\n",
    "            if in_missing:\n",
    "                print(f\"       Missing in in-group file: {[col for col in [r1_col, r2_col] if col not in df_ingroup.columns]}\")\n",
    "            if out_missing:\n",
    "                print(f\"       Missing in out-group file: {[col for col in [r1_col, r2_col] if col not in df_outgroup_source.columns]}\")\n",
    "            continue\n",
    "        \n",
    "        # Process In-group data\n",
    "        in_r1 = pd.to_numeric(in_group_data[r1_col], errors='coerce')\n",
    "        in_r2 = pd.to_numeric(in_group_data[r2_col], errors='coerce')\n",
    "        in_avg = (in_r1 + in_r2) / 2\n",
    "        in_clean = in_avg.dropna()\n",
    "        \n",
    "        # Process Out-group data (recalculate from source)\n",
    "        out_r1 = pd.to_numeric(out_group_data[r1_col], errors='coerce')\n",
    "        out_r2 = pd.to_numeric(out_group_data[r2_col], errors='coerce')\n",
    "        out_avg = (out_r1 + out_r2) / 2\n",
    "        out_clean = out_avg.dropna()\n",
    "        \n",
    "        print(f\"     In-group valid samples: {len(in_clean)}\")\n",
    "        print(f\"     Out-group valid samples: {len(out_clean)}\")\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(in_clean) < 2 or len(out_clean) < 2:\n",
    "            print(f\"     Skipping {measure} due to insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate statistics for both groups\n",
    "        in_stats = {\n",
    "            'mean': in_clean.mean(),\n",
    "            'median': in_clean.median(),\n",
    "            'std': in_clean.std(),\n",
    "            'min': in_clean.min(),\n",
    "            'max': in_clean.max(),\n",
    "            'n': len(in_clean)\n",
    "        }\n",
    "        \n",
    "        out_stats = {\n",
    "            'mean': out_clean.mean(),\n",
    "            'median': out_clean.median(),\n",
    "            'std': out_clean.std(),\n",
    "            'min': out_clean.min(),\n",
    "            'max': out_clean.max(),\n",
    "            'n': len(out_clean)\n",
    "        }\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        try:\n",
    "            statistic, p_value = mannwhitneyu(in_clean, out_clean, alternative='two-sided')\n",
    "            \n",
    "            # Calculate z-score approximation\n",
    "            n1, n2 = len(in_clean), len(out_clean)\n",
    "            mean_u = n1 * n2 / 2\n",
    "            std_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "            z_score = (statistic - mean_u) / std_u\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     Statistical test failed for {measure}: {e}\")\n",
    "            z_score, p_value = 0, 1\n",
    "            continue\n",
    "        \n",
    "        # Determine significance level\n",
    "        if p_value < 0.001:\n",
    "            significance = \"***\"\n",
    "        elif p_value < 0.01:\n",
    "            significance = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            significance = \"*\"\n",
    "        else:\n",
    "            significance = \"ns\"\n",
    "        \n",
    "        print(f\"     In-group: μ={in_stats['mean']:.2f}, median={in_stats['median']:.2f}, n={in_stats['n']}\")\n",
    "        print(f\"     Out-group: μ={out_stats['mean']:.2f}, median={out_stats['median']:.2f}, n={out_stats['n']}\")\n",
    "        print(f\"     Mann-Whitney U: z={z_score:.2f}, p={p_value:.6f} {significance}\")\n",
    "        \n",
    "        # Store results\n",
    "        table_data.append({\n",
    "            'measure': measure,\n",
    "            'z_score': z_score,\n",
    "            'p_value': p_value,\n",
    "            'significance': significance,\n",
    "            'in_stats': in_stats,\n",
    "            'out_stats': out_stats\n",
    "        })\n",
    "        \n",
    "        # Store for CSV output (Table 3 format)\n",
    "        table_rows.extend([\n",
    "            {\n",
    "                'Answer_source': 'In-group',\n",
    "                'Measure': f\"{measure} (z={z_score:.2f}, p={p_value:.6f})\",\n",
    "                'Mean_μ': round(in_stats['mean'], 2),\n",
    "                'Median': round(in_stats['median'], 2),\n",
    "                'SD_σ': round(in_stats['std'], 2),\n",
    "                'Min': round(in_stats['min'], 2),\n",
    "                'Max': round(in_stats['max'], 2)\n",
    "            },\n",
    "            {\n",
    "                'Answer_source': 'Out-group',\n",
    "                'Measure': '',  # Empty for second row\n",
    "                'Mean_μ': round(out_stats['mean'], 2),\n",
    "                'Median': round(out_stats['median'], 2),\n",
    "                'SD_σ': round(out_stats['std'], 2),\n",
    "                'Min': round(out_stats['min'], 2),\n",
    "                'Max': round(out_stats['max'], 2)\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Display results in Table 3 format\n",
    "    print(f\"\\n\" + \"=\"*85)\n",
    "    print(\"TABLE 3: Descriptive statistics comparing in-group and out-group answers\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    for data in table_data:\n",
    "        measure = data['measure']\n",
    "        z_score = data['z_score']\n",
    "        p_value = data['p_value']\n",
    "        significance = data['significance']\n",
    "        in_stats = data['in_stats']\n",
    "        out_stats = data['out_stats']\n",
    "        \n",
    "        # Print measure header with statistics\n",
    "        print(f\"\\n{measure} (z={z_score:.2f}, p={p_value:.6f} {significance})\")\n",
    "        print(\"-\" * 65)\n",
    "        print(f\"{'Answer source':<15} {'Mean (μ)':<10} {'Median':<8} {'SD (σ)':<8} {'Min':<8} {'Max':<8} {'N':<8}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # In-group row\n",
    "        print(f\"{'In-group':<15} {in_stats['mean']:<10.2f} {in_stats['median']:<8.2f} {in_stats['std']:<8.2f} {in_stats['min']:<8.2f} {in_stats['max']:<8.2f} {in_stats['n']:<8}\")\n",
    "        \n",
    "        # Out-group row  \n",
    "        print(f\"{'Out-group':<15} {out_stats['mean']:<10.2f} {out_stats['median']:<8.2f} {out_stats['std']:<8.2f} {out_stats['min']:<8.2f} {out_stats['max']:<8.2f} {out_stats['n']:<8}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    if table_data:\n",
    "        significant_count = sum(1 for data in table_data if data['p_value'] < 0.05)\n",
    "        total_in_group = table_data[0]['in_stats']['n']\n",
    "        total_out_group = table_data[0]['out_stats']['n']\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*85)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*85)\n",
    "        print(f\"Total measures analyzed: {len(table_data)}\")\n",
    "        print(f\"Statistically significant differences (p < 0.05): {significant_count}\")\n",
    "        print(f\"In-group sample size: {total_in_group}\")\n",
    "        print(f\"Out-group sample size: {total_out_group}\")\n",
    "        \n",
    "        print(f\"\\nTable 3. Descriptive statistics for the five outcome measures\")\n",
    "        print(f\"comparing In-group (N={total_in_group}) and out-group (N={total_out_group}) answers for\")\n",
    "        print(f\"cost controlled questions. The z statistic is the Mann-Whitney U score.\")\n",
    "        print(f\"Significance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "    \n",
    "    # Save results to files\n",
    "    if output_dir and table_rows:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save main Table 3 format\n",
    "        table_df = pd.DataFrame(table_rows)\n",
    "        csv_path = os.path.join(output_dir, 'Table3_ingroup_vs_outgroup.csv')\n",
    "        table_df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nTable 3 results saved to: {csv_path}\")\n",
    "        \n",
    "        # Save detailed statistical results\n",
    "        if table_data:\n",
    "            detailed_results = []\n",
    "            for data in table_data:\n",
    "                detailed_results.append({\n",
    "                    'Measure': data['measure'],\n",
    "                    'Mann_Whitney_Z': round(data['z_score'], 2),\n",
    "                    'P_value': round(data['p_value'], 6),\n",
    "                    'Significance': data['significance'],\n",
    "                    'Effect_Direction': 'In-group > Out-group' if data['in_stats']['mean'] > data['out_stats']['mean'] else 'Out-group > In-group',\n",
    "                    'InGroup_Mean': round(data['in_stats']['mean'], 2),\n",
    "                    'InGroup_Median': round(data['in_stats']['median'], 2),\n",
    "                    'InGroup_SD': round(data['in_stats']['std'], 2),\n",
    "                    'InGroup_Min': round(data['in_stats']['min'], 2),\n",
    "                    'InGroup_Max': round(data['in_stats']['max'], 2),\n",
    "                    'InGroup_N': data['in_stats']['n'],\n",
    "                    'OutGroup_Mean': round(data['out_stats']['mean'], 2),\n",
    "                    'OutGroup_Median': round(data['out_stats']['median'], 2),\n",
    "                    'OutGroup_SD': round(data['out_stats']['std'], 2),\n",
    "                    'OutGroup_Min': round(data['out_stats']['min'], 2),\n",
    "                    'OutGroup_Max': round(data['out_stats']['max'], 2),\n",
    "                    'OutGroup_N': data['out_stats']['n']\n",
    "                })\n",
    "            \n",
    "            detailed_df = pd.DataFrame(detailed_results)\n",
    "            detailed_path = os.path.join(output_dir, 'Table3_ingroup_vs_outgroup.csv')\n",
    "            detailed_df.to_csv(detailed_path, index=False)\n",
    "            print(f\"Detailed statistical results saved to: {detailed_path}\")\n",
    "    \n",
    "    return table_data\n",
    "\n",
    "# Usage function\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths - modify these according to your setup\n",
    "    ingroup_file = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\\cost_controlled_ingroup_llm_rating_results_20250728_000829.csv\"\n",
    "    \n",
    "    # This should be the file containing out_group_human data to recalculate statistics\n",
    "    outgroup_source_file = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\\cost_controlled_llm_rating_results_20250728_003329.csv\"\n",
    "    \n",
    "    output_directory = r\"D:\\Wisconsin_Madison\\2025summer\\Dr.Arriaga\\AutismTE\\TE_autism\\rating_results\\final_rating_result\\cost_controlled\\0728\"\n",
    "    \n",
    "    print(\"Starting Complete In-group vs Out-group Analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = analyze_ingroup_vs_outgroup_complete(\n",
    "        ingroup_file=ingroup_file,\n",
    "        outgroup_source_file=outgroup_source_file,\n",
    "        output_dir=output_directory\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nAnalysis completed successfully!\")\n",
    "        print(f\"Generated files:\")\n",
    "        print(\"- Table3_ingroup_vs_outgroup.csv\")\n",
    "        print(\"- Table3_ingroup_vs_outgroup.csv\")\n",
    "    else:\n",
    "        print(f\"\\nAnalysis failed - please check your data files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
